{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b2dfba5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7ec358e8f64e08b03434231be2e64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f94996152243028db5dcbe969d16cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_size = 1000\n",
    "\n",
    "dataset = load_dataset(\"opus_books\", \"en-fr\")[\"train\"] # Take the training part of the dataset\n",
    "dataset = dataset.train_test_split(test_size=0.2)      # Split the dataset into \"train\" and \"test\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # Get the tokenizer\n",
    "\n",
    "# make the tokenize_function\n",
    "def tokenize_function(dataset):\n",
    "    input_outputs = dataset[\"translation\"]\n",
    "    inputs = [input_output[\"en\"] for input_output in input_outputs]\n",
    "    outputs = [input_output[\"fr\"] for input_output in input_outputs]\n",
    "    return tokenizer(inputs, text_target=outputs, truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Make a smaller dataset\n",
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9d727fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100\n",
      "32100\n"
     ]
    }
   ],
   "source": [
    "train_input = small_train_dataset[\"input_ids\"]\n",
    "train_output = small_train_dataset[\"labels\"]\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.vocab_size)\n",
    "source_vocab_size = tokenizer.vocab_size + 2\n",
    "target_vocab_size = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2645a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, source_vocab_size):\n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "                \n",
    "        self.embedding_table = nn.Embedding(source_vocab_size, embedding_dim)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embedding = self.embedding_table(x)\n",
    "        output, (h_n, c_n) = self.lstm_layer(x_embedding)\n",
    "        return output, h_n\n",
    "    \n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "                \n",
    "    def forward(self, decoder_ht, encoder_h):\n",
    "        # decoder_ht: bs * D\n",
    "        # encoder_h: bs * L * D\n",
    "        score = (decoder_ht.unsqueeze(1) * encoder_h).sum(-1)  # e\n",
    "        attention_prob = torch.softmax(score, dim=-1)  # a\n",
    "        context = (attention_prob.unsqueeze(-1) * encoder_h).sum(1)  # c_t\n",
    "        return attention_prob, context\n",
    "    \n",
    "    \n",
    "    \n",
    "class Seq2SeqDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_classes, target_vocab_size, start_id, end_id):\n",
    "        super(Seq2SeqDecoder, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "                \n",
    "        self.embedding_table = nn.Embedding(target_vocab_size, embedding_dim)\n",
    "        self.lstm_cell = nn.LSTMCell(embedding_dim, hidden_dim)  # one single step\n",
    "        self.attention = Attention()\n",
    "        self.proj_layer = nn.Linear(2 * hidden_dim, num_classes)\n",
    "        self.start_id = start_id\n",
    "        self.end_id = end_id\n",
    "\n",
    "    def forward(self, shifted_target_ids, encoder_h):\n",
    "        \n",
    "        shifted_target_embedding = self.embedding_table(shifted_target_ids)\n",
    "        \n",
    "        bs, target_len = shifted_target_ids.shape\n",
    "        source_len = encoder_h.shape[1]\n",
    "        logits = torch.zeros(bs, target_len, self.num_classes)\n",
    "        attention_probs = torch.zeros(bs, target_len, source_len)\n",
    "                \n",
    "        for t in range(0, target_len):\n",
    "            decoder_input_t = shifted_target_embedding[:, t, :]\n",
    "            if t == 0:\n",
    "                decoder_ht, decoder_ct = self.lstm_cell(decoder_input_t)\n",
    "            else:\n",
    "                decoder_ht, decoder_ct = self.lstm_cell(decoder_input_t, (decoder_ht, decoder_ct))\n",
    "            attention_prob, context = self.attention(decoder_ht, encoder_h)   \n",
    "            \n",
    "            hn_and_context = torch.cat((decoder_ht, context), dim=-1)\n",
    "            logit = self.proj_layer(hn_and_context)\n",
    "            \n",
    "            logits[:, t, :] = logit\n",
    "            attention_probs[:, t, :] = attention_prob\n",
    "            \n",
    "        return logits, attention_prob\n",
    "            \n",
    "    def inference(self, encoder_h):\n",
    "        result = [self.start_id]\n",
    "                        \n",
    "        for t in range(0, 128):\n",
    "            decoder_input_t = self.embedding_table(result[-1])[:, 0, :]\n",
    "            if t == 0:\n",
    "                decoder_ht, decoder_ct = self.lstm_cell(decoder_input_t)\n",
    "            else:\n",
    "                decoder_ht, decoder_ct = self.lstm_cell(decoder_input_t, (decoder_ht, decoder_ct))\n",
    "                \n",
    "            attention_prob, context = self.attention(decoder_ht, encoder_h)   \n",
    "            \n",
    "            hn_and_context = torch.cat((decoder_ht, context), dim=-1)\n",
    "            logit = self.proj_layer(hn_and_context)\n",
    "            \n",
    "            target_id = torch.argmax(logit)\n",
    "            if target_id == self.end_id:\n",
    "                break\n",
    "            \n",
    "            result.append(torch.tensor([[target_id]]))\n",
    "            \n",
    "        return torch.stack(result[1:], dim=0)\n",
    "    \n",
    "    \n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_classes, source_vocab_size, target_vocab_size, start_id, end_id):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.encoder = Seq2SeqEncoder(embedding_dim, hidden_dim, source_vocab_size)\n",
    "        self.decoder = Seq2SeqDecoder(embedding_dim, hidden_dim, num_classes, target_vocab_size, start_id, end_id)\n",
    "        \n",
    "    def forward(self, source_ids, shifted_target_ids):\n",
    "        encoder_output, encoder_h = self.encoder(source_ids)\n",
    "        logits, attention_prob = self.decoder(shifted_target_ids, encoder_h)\n",
    "        \n",
    "        return logits, attention_prob\n",
    "    \n",
    "    def inference(self, source_ids):\n",
    "        encoder_output, encoder_h = self.encoder(source_ids)\n",
    "        return self.decoder.inference(encoder_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e1d58047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef16d8ddfe88404789b8fc15aba29593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m             iter_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m     32\u001b[0m         loss_list\u001b[38;5;241m.\u001b[39mappend(iter_loss)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[133], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(iters)\u001b[0m\n\u001b[0;32m     21\u001b[0m target_token   \u001b[38;5;241m=\u001b[39m   torch\u001b[38;5;241m.\u001b[39mtensor(small_train_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m target_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((start_id, target_token[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], end_id), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 24\u001b[0m pred_logits, attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m pred_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(pred_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_logits\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), target_token\u001b[38;5;241m.\u001b[39mlong())\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[132], line 99\u001b[0m, in \u001b[0;36mSeq2SeqModel.forward\u001b[1;34m(self, source_ids, shifted_target_ids)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_ids, shifted_target_ids):\n\u001b[0;32m     98\u001b[0m     encoder_output, encoder_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(source_ids)\n\u001b[1;32m---> 99\u001b[0m     logits, attention_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshifted_target_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, attention_prob\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[132], line 60\u001b[0m, in \u001b[0;36mSeq2SeqDecoder.forward\u001b[1;34m(self, shifted_target_ids, encoder_h)\u001b[0m\n\u001b[0;32m     57\u001b[0m attention_prob, context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(decoder_ht, encoder_h)   \n\u001b[0;32m     59\u001b[0m hn_and_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((decoder_ht, context), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhn_and_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m logits[:, t, :] \u001b[38;5;241m=\u001b[39m logit\n\u001b[0;32m     63\u001b[0m attention_probs[:, t, :] \u001b[38;5;241m=\u001b[39m attention_prob\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = 1000\n",
    "start_id = source_vocab_size - 2\n",
    "end_id = source_vocab_size - 1\n",
    "start_id = start_id * torch.ones(1, 1).long()\n",
    "end_id = end_id * torch.ones(1, 1).long()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Seq2SeqModel(64, 128, target_vocab_size, source_vocab_size, target_vocab_size, start_id, end_id).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "def train(iters = 1000):\n",
    "    for iter in tqdm(range(iters)):\n",
    "        iter_loss = 0\n",
    "        for i in range(train_size):\n",
    "            source_token   =   torch.tensor(small_train_dataset[\"input_ids\"][i]).unsqueeze(0).long().to(device)\n",
    "            target_token   =   torch.tensor(small_train_dataset[\"labels\"][i]).unsqueeze(0).long().to(device)\n",
    "            target_token = torch.cat((start_id, target_token[:, 0:-1], end_id), dim=1).long()\n",
    "            \n",
    "            pred_logits, attention_probs = model(source_token, target_token)\n",
    "            pred_prob = torch.softmax(pred_logits, dim=-1)\n",
    "            loss = criterion(pred_logits.permute(0, 2, 1).float(), target_token.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_loss += loss.data\n",
    "        loss_list.append(iter_loss)\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1185ebef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "Decoder_ht shape: torch.Size([1, 128])\n",
      "Encoder_h shape: torch.Size([1, 1, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nez apropie sprint plin sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These obsThere sprint documentary 27, These Thesenez These'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(input):\n",
    "    source_token = tokenizer.encode(input)\n",
    "    source_token = torch.tensor(source_token).unsqueeze(dim=0).long()\n",
    "    output_id = model.inference(source_token)\n",
    "    output = tokenizer.decode(output_id.squeeze().squeeze())\n",
    "    return output\n",
    "\n",
    "\n",
    "translate(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad5e8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "45a261ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1df26644d90>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fe9e6a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, 1) == torch.ones(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
