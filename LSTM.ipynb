{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32672f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f3b35b",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "60aca4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "L = 32\n",
    "D = 1\n",
    "H_in = 64\n",
    "H_cell = 128\n",
    "H_out = H_cell\n",
    "num_layers = 1\n",
    "\n",
    "input = torch.rand(N, L, H_in) # N * L * D\n",
    "h0 = torch.rand(D*num_layers, N, H_out)\n",
    "c0 = torch.rand(D*num_layers, N, H_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59e04966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 128])\n",
      "torch.Size([1, 2, 128])\n",
      "torch.Size([1, 2, 128])\n",
      "weight_ih_l0\n",
      "weight_hh_l0\n",
      "bias_ih_l0\n",
      "bias_hh_l0\n"
     ]
    }
   ],
   "source": [
    "layer = nn.LSTM(H_in, H_cell, 1, batch_first=True, bidirectional=False)\n",
    "torch_output, (torch_h_n, torch_c_n) = layer(input, (h0, c0))\n",
    "print(torch_output.shape)\n",
    "print(torch_h_n.shape)\n",
    "print(torch_c_n.shape)\n",
    "for k, v in layer.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b55a4041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 128])\n",
      "torch.Size([2, 32, 128])\n",
      "torch.Size([1, 2, 128])\n",
      "torch.Size([1, 2, 128])\n",
      "torch.Size([1, 2, 128])\n",
      "torch.Size([1, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "# we only take the initial_states of one layer\n",
    "def LSTM_forward(input, initial_states, W_ih, W_hh, B_ih, B_hh):\n",
    "    h_0, c_0 = initial_states\n",
    "    \n",
    "    L = input.shape[1]\n",
    "    hidden_dim = W_ih.shape[0] // 4\n",
    "    output = torch.zeros(input.shape[0], input.shape[1], W_ih.shape[0]//4)\n",
    "    h_prev = h_0\n",
    "    c_prev = c_0\n",
    "    \n",
    "    for t in range(0, L):\n",
    "        x_t = input[:, t, :]\n",
    "        \n",
    "        sum_ = x_t @ W_ih.T + B_ih + h_prev @ W_hh.T + B_hh\n",
    "        \n",
    "        it = torch.sigmoid(sum_[:, 0 : hidden_dim])\n",
    "        ft = torch.sigmoid(sum_[:, hidden_dim : 2*hidden_dim])\n",
    "        gt = torch.tanh(sum_[:, 2*hidden_dim : 3*hidden_dim])\n",
    "        ot = torch.sigmoid(sum_[:, 3*hidden_dim : ])\n",
    "\n",
    "        c_prev = ft * c_prev + it * gt\n",
    "        h_prev = ot * torch.tanh(c_prev)\n",
    "        output[:, t, :] = h_prev\n",
    "        \n",
    "    return output, (torch.stack([h_prev]), torch.stack([c_prev]))\n",
    "\n",
    "\n",
    "my_output, (my_h_n, my_c_n) = LSTM_forward(input, (h0[0], c0[0]), layer.weight_ih_l0, layer.weight_hh_l0, layer.bias_ih_l0, layer.bias_hh_l0)\n",
    "# print(my_output)\n",
    "# print(torch_output)\n",
    "# print(my_h_n)\n",
    "# print(torch_h_n)\n",
    "# print(my_c_n)\n",
    "# print(torch_c_n)\n",
    "print(my_output.shape)\n",
    "print(torch_output.shape)\n",
    "print(my_h_n.shape)\n",
    "print(torch_h_n.shape)\n",
    "print(my_c_n.shape)\n",
    "print(torch_c_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f02fb",
   "metadata": {},
   "source": [
    "**Bidirectional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "112c8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "L = 32\n",
    "D = 2 # because it is bidirectional\n",
    "H_in = 64\n",
    "H_cell = 128\n",
    "H_out = H_cell\n",
    "num_layers = 1\n",
    "\n",
    "input = torch.rand(N, L, H_in) # N * L * D\n",
    "h0 = torch.rand(D*num_layers, N, H_out)\n",
    "c0 = torch.rand(D*num_layers, N, H_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "60d77237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 256])\n",
      "torch.Size([2, 2, 128])\n",
      "torch.Size([2, 2, 128])\n",
      "weight_ih_l0\n",
      "weight_hh_l0\n",
      "bias_ih_l0\n",
      "bias_hh_l0\n",
      "weight_ih_l0_reverse\n",
      "weight_hh_l0_reverse\n",
      "bias_ih_l0_reverse\n",
      "bias_hh_l0_reverse\n"
     ]
    }
   ],
   "source": [
    "layer = nn.LSTM(H_in, H_cell, num_layers, batch_first=True, bidirectional=True)\n",
    "torch_output, (torch_h_n, torch_c_n) = layer(input, (h0, c0))\n",
    "print(torch_output.shape)\n",
    "print(torch_h_n.shape)\n",
    "print(torch_c_n.shape)\n",
    "for k, v in layer.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a1f93f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1741, -0.0336,  0.1421,  ..., -0.1452,  0.2346, -0.0763],\n",
      "         [ 0.0464, -0.0366,  0.0995,  ..., -0.1819,  0.2252, -0.0486],\n",
      "         [ 0.0178, -0.0497,  0.0449,  ..., -0.1323,  0.2441, -0.0690],\n",
      "         ...,\n",
      "         [ 0.0156, -0.1277,  0.0246,  ..., -0.0953,  0.2692, -0.0467],\n",
      "         [ 0.0059, -0.1244,  0.0263,  ..., -0.0844,  0.2678, -0.0483],\n",
      "         [ 0.0160, -0.0961, -0.0162,  ..., -0.0085,  0.2007, -0.0068]],\n",
      "\n",
      "        [[ 0.2781, -0.0514,  0.0826,  ..., -0.1563,  0.2397, -0.0396],\n",
      "         [ 0.0668, -0.0968,  0.0734,  ..., -0.1255,  0.2682, -0.0106],\n",
      "         [ 0.0387, -0.1219,  0.0125,  ..., -0.1425,  0.2543, -0.0961],\n",
      "         ...,\n",
      "         [-0.0287, -0.0612,  0.0715,  ..., -0.1488,  0.2387,  0.0297],\n",
      "         [ 0.0024, -0.0624,  0.0432,  ..., -0.1322,  0.2051,  0.0322],\n",
      "         [ 0.0312, -0.1018,  0.0526,  ..., -0.0866,  0.2448,  0.1347]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor([[[ 0.1741, -0.0336,  0.1421,  ..., -0.1452,  0.2346, -0.0763],\n",
      "         [ 0.0464, -0.0366,  0.0995,  ..., -0.1819,  0.2252, -0.0486],\n",
      "         [ 0.0178, -0.0497,  0.0449,  ..., -0.1323,  0.2441, -0.0690],\n",
      "         ...,\n",
      "         [ 0.0156, -0.1277,  0.0246,  ..., -0.0953,  0.2692, -0.0467],\n",
      "         [ 0.0059, -0.1244,  0.0263,  ..., -0.0844,  0.2678, -0.0483],\n",
      "         [ 0.0160, -0.0961, -0.0162,  ..., -0.0085,  0.2007, -0.0068]],\n",
      "\n",
      "        [[ 0.2781, -0.0514,  0.0826,  ..., -0.1563,  0.2397, -0.0396],\n",
      "         [ 0.0668, -0.0968,  0.0734,  ..., -0.1255,  0.2682, -0.0106],\n",
      "         [ 0.0387, -0.1219,  0.0125,  ..., -0.1425,  0.2543, -0.0961],\n",
      "         ...,\n",
      "         [-0.0287, -0.0612,  0.0715,  ..., -0.1488,  0.2387,  0.0297],\n",
      "         [ 0.0024, -0.0624,  0.0432,  ..., -0.1322,  0.2051,  0.0322],\n",
      "         [ 0.0312, -0.1018,  0.0526,  ..., -0.0866,  0.2448,  0.1347]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.0160, -0.0961, -0.0162,  0.0092,  0.0969,  0.1409,  0.1323,\n",
      "          -0.0803,  0.0953,  0.0813,  0.1307, -0.1264, -0.0996,  0.1079,\n",
      "          -0.0843,  0.1750, -0.1387,  0.0888, -0.0296, -0.0792,  0.1449,\n",
      "          -0.0547, -0.1030, -0.2571,  0.0660,  0.1585,  0.3083, -0.1816,\n",
      "          -0.0286, -0.0280,  0.0940, -0.0393,  0.1353,  0.1721, -0.0596,\n",
      "          -0.1791,  0.0559, -0.0592,  0.0909,  0.0370,  0.0297, -0.0202,\n",
      "           0.2266,  0.0488,  0.2207,  0.0873,  0.0103, -0.1779,  0.0468,\n",
      "           0.0204, -0.0018, -0.0369,  0.0931,  0.0811, -0.0036,  0.0066,\n",
      "          -0.0572, -0.0153, -0.1771, -0.0771, -0.2323, -0.2007, -0.1744,\n",
      "           0.0518, -0.1663, -0.2067,  0.0166,  0.0870, -0.1518,  0.0602,\n",
      "           0.0910,  0.1376, -0.0781, -0.1955,  0.0263,  0.0264, -0.0475,\n",
      "          -0.0032,  0.0054,  0.1373,  0.1806,  0.1130,  0.0171, -0.1162,\n",
      "           0.1209,  0.2479,  0.0078, -0.0402, -0.0303,  0.1700, -0.1756,\n",
      "           0.1736, -0.1084,  0.1112,  0.0205, -0.1513, -0.0607,  0.1464,\n",
      "           0.1900,  0.0589, -0.0042, -0.0448, -0.1322, -0.1797,  0.0442,\n",
      "           0.0918,  0.0202, -0.0449,  0.1091, -0.0060, -0.1270, -0.0697,\n",
      "          -0.0223,  0.1467,  0.0728, -0.0601, -0.0386, -0.1816,  0.0537,\n",
      "           0.2825,  0.0778,  0.0269, -0.0652, -0.0219,  0.0805,  0.0917,\n",
      "          -0.0857, -0.1300],\n",
      "         [ 0.0312, -0.1018,  0.0526, -0.0268,  0.0857,  0.1382,  0.0836,\n",
      "          -0.1479,  0.1204,  0.0854,  0.0428, -0.0739, -0.1132,  0.1652,\n",
      "          -0.0246,  0.1089, -0.1393,  0.0846, -0.0965, -0.1126,  0.0824,\n",
      "          -0.0774, -0.1360, -0.2537,  0.0676,  0.1352,  0.2860, -0.1474,\n",
      "          -0.0274, -0.0054,  0.0918, -0.0269,  0.0867,  0.1677, -0.0155,\n",
      "          -0.0963,  0.0775, -0.0660,  0.1570, -0.0137,  0.0611, -0.0344,\n",
      "           0.1633,  0.0643,  0.1844, -0.0284,  0.0058, -0.2043,  0.0718,\n",
      "          -0.1034, -0.0981, -0.0696,  0.1595,  0.1863,  0.0488, -0.0886,\n",
      "          -0.0214,  0.0677, -0.2072, -0.0855, -0.1840, -0.2106, -0.2098,\n",
      "           0.0783, -0.1404, -0.1878,  0.0023,  0.0908, -0.1093,  0.1002,\n",
      "           0.0656,  0.2029, -0.1514, -0.1582,  0.0262,  0.0171, -0.0523,\n",
      "           0.0069, -0.0046,  0.0981,  0.1979,  0.1147,  0.0311, -0.1008,\n",
      "           0.0964,  0.2096, -0.0522,  0.0160, -0.0816,  0.1474, -0.1742,\n",
      "           0.1791, -0.0655,  0.0398,  0.0518, -0.0942, -0.1438,  0.1206,\n",
      "           0.1378, -0.0003,  0.0393,  0.0141, -0.1179, -0.0514,  0.0593,\n",
      "           0.0533, -0.0463, -0.0134,  0.0586,  0.0012, -0.2984, -0.0905,\n",
      "           0.0160,  0.1626,  0.1075, -0.0419, -0.0339, -0.1641,  0.0349,\n",
      "           0.2252,  0.0987,  0.0762, -0.0370, -0.0574,  0.0748,  0.0725,\n",
      "          -0.1111, -0.1584]],\n",
      "\n",
      "        [[-0.1241,  0.1052,  0.2094,  0.2028, -0.0899,  0.0714, -0.1503,\n",
      "           0.0647, -0.1571, -0.2381,  0.1226,  0.1378, -0.0657,  0.0542,\n",
      "          -0.0078,  0.0199,  0.1537,  0.1291, -0.0218,  0.1264,  0.0372,\n",
      "           0.0732,  0.1261, -0.0014, -0.0258,  0.1289, -0.0567,  0.0093,\n",
      "          -0.0792,  0.0138,  0.0575, -0.0280, -0.0956, -0.0005, -0.0571,\n",
      "           0.0460,  0.0495,  0.1020,  0.0329,  0.0911,  0.0074,  0.2246,\n",
      "          -0.1844,  0.0332,  0.1351, -0.0935, -0.1546,  0.0651, -0.1030,\n",
      "          -0.0527, -0.1394, -0.0283, -0.0181, -0.0703, -0.0054,  0.1818,\n",
      "           0.1455, -0.0834,  0.0532,  0.1846, -0.0249,  0.0661,  0.0065,\n",
      "           0.1797, -0.0223,  0.1145, -0.0194,  0.0293,  0.0109,  0.0137,\n",
      "          -0.1500, -0.0714,  0.0064,  0.0041,  0.2404,  0.0113, -0.1000,\n",
      "           0.2138,  0.0549, -0.0309,  0.1365,  0.0545,  0.2069,  0.1179,\n",
      "           0.0459, -0.0296,  0.2154, -0.0525, -0.1378,  0.0573,  0.0011,\n",
      "           0.0022, -0.1226, -0.0468,  0.1994, -0.0757, -0.0648,  0.0289,\n",
      "           0.2028,  0.0157, -0.0996,  0.1179,  0.0902,  0.0433, -0.0969,\n",
      "          -0.2194,  0.2204,  0.1459, -0.0313,  0.0564, -0.0118,  0.0207,\n",
      "           0.1197, -0.1017,  0.0200, -0.1615, -0.0952,  0.0359, -0.0314,\n",
      "          -0.0220,  0.0334,  0.2044, -0.0940, -0.0572,  0.1461, -0.1452,\n",
      "           0.2346, -0.0763],\n",
      "         [-0.1338,  0.2666,  0.1607,  0.2310, -0.0552,  0.1701, -0.1873,\n",
      "           0.0845, -0.1140, -0.1473,  0.1285,  0.0172, -0.0698, -0.0463,\n",
      "           0.0541,  0.0225,  0.1422,  0.1371, -0.0193,  0.1657,  0.0518,\n",
      "           0.0831,  0.0641,  0.0458, -0.0293,  0.1769, -0.0141,  0.0075,\n",
      "          -0.0253,  0.0179,  0.0401,  0.0479,  0.0119,  0.0264, -0.0676,\n",
      "           0.0206,  0.0700,  0.0962,  0.1115,  0.0517, -0.0202,  0.2037,\n",
      "          -0.2468,  0.0413,  0.1372, -0.1212, -0.2082, -0.0007, -0.1795,\n",
      "          -0.0789, -0.2093, -0.0033, -0.0361, -0.1256, -0.0249,  0.1170,\n",
      "           0.1566, -0.0502,  0.0040,  0.2062, -0.1703,  0.1005,  0.0295,\n",
      "           0.1845, -0.0126,  0.0540, -0.0088,  0.0659,  0.0583,  0.0057,\n",
      "          -0.1637, -0.0204, -0.0180,  0.0542,  0.1395,  0.0402, -0.0778,\n",
      "           0.1710, -0.0598, -0.0196,  0.1275,  0.0005,  0.1553,  0.0831,\n",
      "           0.0698,  0.0010,  0.1272, -0.1644, -0.0701,  0.0853, -0.0221,\n",
      "           0.0288, -0.0805, -0.0602,  0.2222, -0.0040, -0.0901,  0.0455,\n",
      "           0.2215, -0.0089, -0.1269,  0.1168,  0.0638,  0.0566, -0.0740,\n",
      "          -0.1891,  0.2113,  0.0567,  0.0527,  0.0319,  0.0822, -0.0382,\n",
      "           0.1000, -0.1303,  0.0859, -0.1720, -0.1212,  0.0104, -0.0637,\n",
      "           0.0245,  0.0266,  0.1630, -0.1121,  0.0008,  0.1038, -0.1563,\n",
      "           0.2397, -0.0396]]], grad_fn=<CatBackward0>)\n",
      "tensor([[[ 0.0160, -0.0961, -0.0162,  0.0092,  0.0969,  0.1409,  0.1323,\n",
      "          -0.0803,  0.0953,  0.0813,  0.1307, -0.1264, -0.0996,  0.1079,\n",
      "          -0.0843,  0.1750, -0.1387,  0.0888, -0.0296, -0.0792,  0.1449,\n",
      "          -0.0547, -0.1030, -0.2571,  0.0660,  0.1585,  0.3083, -0.1816,\n",
      "          -0.0286, -0.0280,  0.0940, -0.0393,  0.1353,  0.1721, -0.0596,\n",
      "          -0.1791,  0.0559, -0.0592,  0.0909,  0.0370,  0.0297, -0.0202,\n",
      "           0.2266,  0.0488,  0.2207,  0.0873,  0.0103, -0.1779,  0.0468,\n",
      "           0.0204, -0.0018, -0.0369,  0.0931,  0.0811, -0.0036,  0.0066,\n",
      "          -0.0572, -0.0153, -0.1771, -0.0771, -0.2323, -0.2007, -0.1744,\n",
      "           0.0518, -0.1663, -0.2067,  0.0166,  0.0870, -0.1518,  0.0602,\n",
      "           0.0910,  0.1376, -0.0781, -0.1955,  0.0263,  0.0264, -0.0475,\n",
      "          -0.0032,  0.0054,  0.1373,  0.1806,  0.1130,  0.0171, -0.1162,\n",
      "           0.1209,  0.2479,  0.0078, -0.0402, -0.0303,  0.1700, -0.1756,\n",
      "           0.1736, -0.1084,  0.1112,  0.0205, -0.1513, -0.0607,  0.1464,\n",
      "           0.1900,  0.0589, -0.0042, -0.0448, -0.1322, -0.1797,  0.0442,\n",
      "           0.0918,  0.0202, -0.0449,  0.1091, -0.0060, -0.1270, -0.0697,\n",
      "          -0.0223,  0.1467,  0.0728, -0.0601, -0.0386, -0.1816,  0.0537,\n",
      "           0.2825,  0.0778,  0.0269, -0.0652, -0.0219,  0.0805,  0.0917,\n",
      "          -0.0857, -0.1300],\n",
      "         [ 0.0312, -0.1018,  0.0526, -0.0268,  0.0857,  0.1382,  0.0836,\n",
      "          -0.1479,  0.1204,  0.0854,  0.0428, -0.0739, -0.1132,  0.1652,\n",
      "          -0.0246,  0.1089, -0.1393,  0.0846, -0.0965, -0.1126,  0.0824,\n",
      "          -0.0774, -0.1360, -0.2537,  0.0676,  0.1352,  0.2860, -0.1474,\n",
      "          -0.0274, -0.0054,  0.0918, -0.0269,  0.0867,  0.1677, -0.0155,\n",
      "          -0.0963,  0.0775, -0.0660,  0.1570, -0.0137,  0.0611, -0.0344,\n",
      "           0.1633,  0.0643,  0.1844, -0.0284,  0.0058, -0.2043,  0.0718,\n",
      "          -0.1034, -0.0981, -0.0696,  0.1595,  0.1863,  0.0488, -0.0886,\n",
      "          -0.0214,  0.0677, -0.2072, -0.0855, -0.1840, -0.2106, -0.2098,\n",
      "           0.0783, -0.1404, -0.1878,  0.0023,  0.0908, -0.1093,  0.1002,\n",
      "           0.0656,  0.2029, -0.1514, -0.1582,  0.0262,  0.0171, -0.0523,\n",
      "           0.0069, -0.0046,  0.0981,  0.1979,  0.1147,  0.0311, -0.1008,\n",
      "           0.0964,  0.2096, -0.0522,  0.0160, -0.0816,  0.1474, -0.1742,\n",
      "           0.1791, -0.0655,  0.0398,  0.0518, -0.0942, -0.1438,  0.1206,\n",
      "           0.1378, -0.0003,  0.0393,  0.0141, -0.1179, -0.0514,  0.0593,\n",
      "           0.0533, -0.0463, -0.0134,  0.0586,  0.0012, -0.2984, -0.0905,\n",
      "           0.0160,  0.1626,  0.1075, -0.0419, -0.0339, -0.1641,  0.0349,\n",
      "           0.2252,  0.0987,  0.0762, -0.0370, -0.0574,  0.0748,  0.0725,\n",
      "          -0.1111, -0.1584]],\n",
      "\n",
      "        [[-0.1241,  0.1052,  0.2094,  0.2028, -0.0899,  0.0714, -0.1503,\n",
      "           0.0647, -0.1571, -0.2381,  0.1226,  0.1378, -0.0657,  0.0542,\n",
      "          -0.0078,  0.0199,  0.1537,  0.1291, -0.0218,  0.1264,  0.0372,\n",
      "           0.0732,  0.1261, -0.0014, -0.0258,  0.1289, -0.0567,  0.0093,\n",
      "          -0.0792,  0.0138,  0.0575, -0.0280, -0.0956, -0.0005, -0.0571,\n",
      "           0.0460,  0.0495,  0.1020,  0.0329,  0.0911,  0.0074,  0.2246,\n",
      "          -0.1844,  0.0332,  0.1351, -0.0935, -0.1546,  0.0651, -0.1030,\n",
      "          -0.0527, -0.1394, -0.0283, -0.0181, -0.0703, -0.0054,  0.1818,\n",
      "           0.1455, -0.0834,  0.0532,  0.1846, -0.0249,  0.0661,  0.0065,\n",
      "           0.1797, -0.0223,  0.1145, -0.0194,  0.0293,  0.0109,  0.0137,\n",
      "          -0.1500, -0.0714,  0.0064,  0.0041,  0.2404,  0.0113, -0.1000,\n",
      "           0.2138,  0.0549, -0.0309,  0.1365,  0.0545,  0.2069,  0.1179,\n",
      "           0.0459, -0.0296,  0.2154, -0.0525, -0.1378,  0.0573,  0.0011,\n",
      "           0.0022, -0.1226, -0.0468,  0.1994, -0.0757, -0.0648,  0.0289,\n",
      "           0.2028,  0.0157, -0.0996,  0.1179,  0.0902,  0.0433, -0.0969,\n",
      "          -0.2194,  0.2204,  0.1459, -0.0313,  0.0564, -0.0118,  0.0207,\n",
      "           0.1197, -0.1017,  0.0200, -0.1615, -0.0952,  0.0359, -0.0314,\n",
      "          -0.0220,  0.0334,  0.2044, -0.0940, -0.0572,  0.1461, -0.1452,\n",
      "           0.2346, -0.0763],\n",
      "         [-0.1338,  0.2666,  0.1607,  0.2310, -0.0552,  0.1701, -0.1873,\n",
      "           0.0845, -0.1140, -0.1473,  0.1285,  0.0172, -0.0698, -0.0463,\n",
      "           0.0541,  0.0225,  0.1422,  0.1371, -0.0193,  0.1657,  0.0518,\n",
      "           0.0831,  0.0641,  0.0458, -0.0293,  0.1769, -0.0141,  0.0075,\n",
      "          -0.0253,  0.0179,  0.0401,  0.0479,  0.0119,  0.0264, -0.0676,\n",
      "           0.0206,  0.0700,  0.0962,  0.1115,  0.0517, -0.0202,  0.2037,\n",
      "          -0.2468,  0.0413,  0.1372, -0.1212, -0.2082, -0.0007, -0.1795,\n",
      "          -0.0789, -0.2093, -0.0033, -0.0361, -0.1256, -0.0249,  0.1170,\n",
      "           0.1566, -0.0502,  0.0040,  0.2062, -0.1703,  0.1005,  0.0295,\n",
      "           0.1845, -0.0126,  0.0540, -0.0088,  0.0659,  0.0583,  0.0057,\n",
      "          -0.1637, -0.0204, -0.0180,  0.0542,  0.1395,  0.0402, -0.0778,\n",
      "           0.1710, -0.0598, -0.0196,  0.1275,  0.0005,  0.1553,  0.0831,\n",
      "           0.0698,  0.0010,  0.1272, -0.1644, -0.0701,  0.0853, -0.0221,\n",
      "           0.0288, -0.0805, -0.0602,  0.2222, -0.0040, -0.0901,  0.0455,\n",
      "           0.2215, -0.0089, -0.1269,  0.1168,  0.0638,  0.0566, -0.0740,\n",
      "          -0.1891,  0.2113,  0.0567,  0.0527,  0.0319,  0.0822, -0.0382,\n",
      "           0.1000, -0.1303,  0.0859, -0.1720, -0.1212,  0.0104, -0.0637,\n",
      "           0.0245,  0.0266,  0.1630, -0.1121,  0.0008,  0.1038, -0.1563,\n",
      "           0.2397, -0.0396]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 3.4039e-02, -2.0610e-01, -3.7627e-02,  1.6435e-02,  2.1489e-01,\n",
      "           2.9010e-01,  3.5197e-01, -1.6314e-01,  1.8321e-01,  1.8972e-01,\n",
      "           2.8945e-01, -2.2918e-01, -1.7280e-01,  2.6768e-01, -1.8206e-01,\n",
      "           4.1356e-01, -3.1009e-01,  2.0977e-01, -6.2950e-02, -1.8638e-01,\n",
      "           2.8613e-01, -1.7454e-01, -2.2555e-01, -4.8036e-01,  1.3407e-01,\n",
      "           3.7522e-01,  5.4177e-01, -3.1850e-01, -5.6688e-02, -5.9533e-02,\n",
      "           2.2795e-01, -7.7434e-02,  2.8810e-01,  3.5188e-01, -1.4213e-01,\n",
      "          -3.5754e-01,  1.1210e-01, -1.3314e-01,  2.0075e-01,  6.5054e-02,\n",
      "           6.2503e-02, -3.4494e-02,  4.3014e-01,  1.0921e-01,  4.1285e-01,\n",
      "           1.7532e-01,  2.3299e-02, -3.7437e-01,  1.0147e-01,  3.5698e-02,\n",
      "          -3.6542e-03, -7.5018e-02,  2.0874e-01,  1.7238e-01, -7.2158e-03,\n",
      "           1.2073e-02, -1.1978e-01, -3.1271e-02, -2.9282e-01, -1.5047e-01,\n",
      "          -3.8412e-01, -3.9620e-01, -4.0160e-01,  1.4331e-01, -3.9360e-01,\n",
      "          -4.1792e-01,  3.4315e-02,  1.4620e-01, -4.7135e-01,  1.0824e-01,\n",
      "           1.9507e-01,  2.7674e-01, -1.8070e-01, -3.5843e-01,  5.1957e-02,\n",
      "           6.5116e-02, -1.0482e-01, -6.6544e-03,  1.1303e-02,  3.1136e-01,\n",
      "           3.1630e-01,  2.3211e-01,  4.0920e-02, -2.1501e-01,  2.5296e-01,\n",
      "           5.4986e-01,  1.4741e-02, -8.4946e-02, -5.6064e-02,  3.6145e-01,\n",
      "          -3.4820e-01,  4.0329e-01, -2.3584e-01,  2.2261e-01,  3.4917e-02,\n",
      "          -2.8986e-01, -1.2381e-01,  3.6250e-01,  3.2494e-01,  9.8000e-02,\n",
      "          -6.6426e-03, -1.0888e-01, -2.7173e-01, -3.3928e-01,  9.0723e-02,\n",
      "           1.8418e-01,  5.0582e-02, -1.2055e-01,  2.0626e-01, -1.0931e-02,\n",
      "          -2.4067e-01, -1.5237e-01, -5.2173e-02,  3.4522e-01,  1.4470e-01,\n",
      "          -1.2350e-01, -7.0452e-02, -3.9412e-01,  1.1495e-01,  5.3261e-01,\n",
      "           1.6105e-01,  4.8677e-02, -1.3728e-01, -4.0478e-02,  1.5165e-01,\n",
      "           1.9992e-01, -2.5297e-01, -2.2562e-01],\n",
      "         [ 6.9101e-02, -2.1846e-01,  1.3369e-01, -4.9444e-02,  1.6715e-01,\n",
      "           2.8393e-01,  2.2679e-01, -2.9722e-01,  2.4258e-01,  1.9142e-01,\n",
      "           1.0462e-01, -1.4125e-01, -2.5307e-01,  3.8116e-01, -5.4459e-02,\n",
      "           2.8649e-01, -3.2456e-01,  2.4105e-01, -2.1641e-01, -2.3955e-01,\n",
      "           1.7323e-01, -2.1895e-01, -3.2445e-01, -4.8006e-01,  1.4712e-01,\n",
      "           3.4940e-01,  5.0933e-01, -2.4338e-01, -5.7652e-02, -1.1761e-02,\n",
      "           2.3883e-01, -5.4607e-02,  1.7813e-01,  3.7565e-01, -3.4085e-02,\n",
      "          -2.0065e-01,  1.5212e-01, -1.4031e-01,  3.4623e-01, -2.5120e-02,\n",
      "           1.2741e-01, -6.1752e-02,  3.0878e-01,  1.3083e-01,  3.7138e-01,\n",
      "          -5.5579e-02,  1.2994e-02, -4.4959e-01,  1.5613e-01, -1.8079e-01,\n",
      "          -1.9671e-01, -1.3032e-01,  3.5197e-01,  3.8525e-01,  8.7489e-02,\n",
      "          -1.5742e-01, -4.3621e-02,  1.7523e-01, -3.7780e-01, -1.6787e-01,\n",
      "          -3.3497e-01, -4.2647e-01, -4.5050e-01,  1.9346e-01, -3.6656e-01,\n",
      "          -3.9096e-01,  4.7682e-03,  1.5169e-01, -3.3073e-01,  1.7162e-01,\n",
      "           1.3369e-01,  4.2995e-01, -3.0744e-01, -3.0849e-01,  5.3905e-02,\n",
      "           4.4929e-02, -1.1513e-01,  1.3015e-02, -9.6739e-03,  2.4341e-01,\n",
      "           3.4841e-01,  2.4598e-01,  6.8973e-02, -2.0587e-01,  2.2697e-01,\n",
      "           4.0024e-01, -9.2038e-02,  3.6523e-02, -1.4286e-01,  3.6884e-01,\n",
      "          -4.0241e-01,  4.7509e-01, -1.4440e-01,  7.4449e-02,  1.0598e-01,\n",
      "          -1.9055e-01, -3.2765e-01,  2.7219e-01,  2.4619e-01, -4.9504e-04,\n",
      "           6.1289e-02,  2.9555e-02, -2.5126e-01, -1.0201e-01,  1.2919e-01,\n",
      "           1.0291e-01, -1.1817e-01, -3.3204e-02,  1.2264e-01,  2.2226e-03,\n",
      "          -5.0340e-01, -1.9218e-01,  3.5726e-02,  3.6148e-01,  2.0753e-01,\n",
      "          -9.6278e-02, -5.6292e-02, -3.9057e-01,  8.5633e-02,  4.1709e-01,\n",
      "           1.9292e-01,  1.4890e-01, -7.5771e-02, -1.0695e-01,  1.5739e-01,\n",
      "           1.3537e-01, -3.2175e-01, -2.4750e-01]],\n",
      "\n",
      "        [[-2.1331e-01,  1.9870e-01,  4.2747e-01,  3.7002e-01, -1.8015e-01,\n",
      "           1.4479e-01, -3.7716e-01,  1.1508e-01, -4.1583e-01, -4.9053e-01,\n",
      "           2.5040e-01,  2.6824e-01, -1.5021e-01,  8.3107e-02, -1.5726e-02,\n",
      "           4.8377e-02,  2.8989e-01,  3.7175e-01, -5.5969e-02,  3.1935e-01,\n",
      "           7.5849e-02,  1.4834e-01,  2.2161e-01, -3.0630e-03, -7.0001e-02,\n",
      "           2.2173e-01, -1.2190e-01,  2.0312e-02, -1.3529e-01,  2.3964e-02,\n",
      "           1.2557e-01, -5.4378e-02, -1.8953e-01, -1.0969e-03, -1.0328e-01,\n",
      "           1.1161e-01,  1.0356e-01,  2.3028e-01,  7.0496e-02,  1.5581e-01,\n",
      "           1.5326e-02,  4.3574e-01, -3.8182e-01,  6.6456e-02,  2.4199e-01,\n",
      "          -2.1421e-01, -3.0467e-01,  1.1622e-01, -1.8367e-01, -1.4434e-01,\n",
      "          -3.4411e-01, -6.9080e-02, -3.5544e-02, -1.2473e-01, -1.0932e-02,\n",
      "           4.2548e-01,  2.7103e-01, -1.6970e-01,  1.0547e-01,  3.2401e-01,\n",
      "          -4.9586e-02,  1.1410e-01,  1.2913e-02,  3.5304e-01, -5.0215e-02,\n",
      "           2.1166e-01, -4.9258e-02,  7.2665e-02,  1.9668e-02,  2.2123e-02,\n",
      "          -2.7149e-01, -1.4019e-01,  1.3606e-02,  8.5011e-03,  4.4455e-01,\n",
      "           2.4961e-02, -2.2516e-01,  3.8911e-01,  1.0631e-01, -8.6187e-02,\n",
      "           3.1378e-01,  1.0800e-01,  4.1232e-01,  2.6003e-01,  8.6265e-02,\n",
      "          -5.9879e-02,  4.3661e-01, -1.0003e-01, -2.1758e-01,  1.0842e-01,\n",
      "           1.8513e-03,  4.2252e-03, -2.6209e-01, -1.0360e-01,  4.1178e-01,\n",
      "          -1.3997e-01, -1.3224e-01,  6.0028e-02,  4.1156e-01,  3.3367e-02,\n",
      "          -1.6979e-01,  2.4294e-01,  1.6355e-01,  9.7499e-02, -2.0043e-01,\n",
      "          -5.1673e-01,  4.6941e-01,  2.7052e-01, -7.6219e-02,  1.1453e-01,\n",
      "          -2.2307e-02,  5.4295e-02,  2.5524e-01, -1.9276e-01,  4.1217e-02,\n",
      "          -3.6951e-01, -2.2598e-01,  7.6875e-02, -7.4192e-02, -5.5125e-02,\n",
      "           7.9114e-02,  3.8219e-01, -1.9498e-01, -1.1813e-01,  3.4182e-01,\n",
      "          -4.1082e-01,  4.0259e-01, -1.4475e-01],\n",
      "         [-2.7099e-01,  5.4715e-01,  3.2521e-01,  4.5966e-01, -1.1112e-01,\n",
      "           3.7622e-01, -4.8489e-01,  1.7667e-01, -2.7844e-01, -2.9416e-01,\n",
      "           2.6203e-01,  3.4473e-02, -1.6417e-01, -7.0460e-02,  1.1014e-01,\n",
      "           6.2760e-02,  2.7803e-01,  3.6600e-01, -5.2252e-02,  3.8604e-01,\n",
      "           1.0027e-01,  2.1057e-01,  1.2173e-01,  8.9060e-02, -7.5294e-02,\n",
      "           3.2185e-01, -2.7169e-02,  1.6036e-02, -4.3379e-02,  3.1025e-02,\n",
      "           9.9505e-02,  9.1407e-02,  2.4055e-02,  4.5617e-02, -1.1436e-01,\n",
      "           5.3148e-02,  1.3466e-01,  2.4727e-01,  2.4024e-01,  8.6377e-02,\n",
      "          -4.1214e-02,  3.7716e-01, -5.3285e-01,  8.4727e-02,  2.6549e-01,\n",
      "          -2.6632e-01, -3.9978e-01, -1.3703e-03, -3.2366e-01, -2.2581e-01,\n",
      "          -5.6472e-01, -8.4482e-03, -7.5477e-02, -2.3586e-01, -4.8572e-02,\n",
      "           2.8875e-01,  3.1725e-01, -1.2073e-01,  9.0915e-03,  4.1182e-01,\n",
      "          -3.2191e-01,  1.7112e-01,  5.0571e-02,  4.2083e-01, -2.7664e-02,\n",
      "           9.5768e-02, -1.9716e-02,  1.4281e-01,  1.1358e-01,  9.8568e-03,\n",
      "          -3.3445e-01, -4.5472e-02, -3.6642e-02,  1.2086e-01,  2.5579e-01,\n",
      "           8.3628e-02, -1.7482e-01,  3.2272e-01, -1.2116e-01, -5.3270e-02,\n",
      "           2.6483e-01,  8.5493e-04,  3.1274e-01,  1.9269e-01,  1.1894e-01,\n",
      "           2.0334e-03,  2.7067e-01, -2.9806e-01, -1.1499e-01,  1.8787e-01,\n",
      "          -4.2089e-02,  5.4020e-02, -1.6258e-01, -1.2875e-01,  4.0621e-01,\n",
      "          -7.7317e-03, -1.7007e-01,  9.9990e-02,  4.0342e-01, -1.9598e-02,\n",
      "          -2.2353e-01,  2.4768e-01,  1.3346e-01,  1.2574e-01, -1.4097e-01,\n",
      "          -4.5026e-01,  4.4853e-01,  9.9196e-02,  1.2615e-01,  5.7220e-02,\n",
      "           1.5482e-01, -9.8447e-02,  1.9001e-01, -2.2765e-01,  1.7198e-01,\n",
      "          -3.5527e-01, -2.3754e-01,  2.3564e-02, -1.5237e-01,  5.6046e-02,\n",
      "           6.4004e-02,  3.2266e-01, -2.2660e-01,  1.4967e-03,  2.1200e-01,\n",
      "          -3.7083e-01,  4.4455e-01, -7.0950e-02]]], grad_fn=<CatBackward0>)\n",
      "tensor([[[ 3.4039e-02, -2.0610e-01, -3.7627e-02,  1.6435e-02,  2.1489e-01,\n",
      "           2.9010e-01,  3.5197e-01, -1.6314e-01,  1.8321e-01,  1.8972e-01,\n",
      "           2.8945e-01, -2.2918e-01, -1.7280e-01,  2.6768e-01, -1.8206e-01,\n",
      "           4.1356e-01, -3.1009e-01,  2.0977e-01, -6.2950e-02, -1.8638e-01,\n",
      "           2.8613e-01, -1.7454e-01, -2.2555e-01, -4.8036e-01,  1.3407e-01,\n",
      "           3.7522e-01,  5.4177e-01, -3.1850e-01, -5.6688e-02, -5.9533e-02,\n",
      "           2.2795e-01, -7.7434e-02,  2.8810e-01,  3.5188e-01, -1.4213e-01,\n",
      "          -3.5754e-01,  1.1210e-01, -1.3314e-01,  2.0075e-01,  6.5054e-02,\n",
      "           6.2503e-02, -3.4494e-02,  4.3014e-01,  1.0921e-01,  4.1285e-01,\n",
      "           1.7532e-01,  2.3299e-02, -3.7437e-01,  1.0147e-01,  3.5698e-02,\n",
      "          -3.6541e-03, -7.5018e-02,  2.0874e-01,  1.7238e-01, -7.2159e-03,\n",
      "           1.2073e-02, -1.1978e-01, -3.1271e-02, -2.9282e-01, -1.5047e-01,\n",
      "          -3.8412e-01, -3.9620e-01, -4.0160e-01,  1.4331e-01, -3.9360e-01,\n",
      "          -4.1792e-01,  3.4315e-02,  1.4620e-01, -4.7135e-01,  1.0824e-01,\n",
      "           1.9507e-01,  2.7674e-01, -1.8070e-01, -3.5843e-01,  5.1957e-02,\n",
      "           6.5116e-02, -1.0482e-01, -6.6544e-03,  1.1303e-02,  3.1136e-01,\n",
      "           3.1630e-01,  2.3211e-01,  4.0920e-02, -2.1501e-01,  2.5296e-01,\n",
      "           5.4986e-01,  1.4741e-02, -8.4946e-02, -5.6064e-02,  3.6145e-01,\n",
      "          -3.4820e-01,  4.0329e-01, -2.3584e-01,  2.2261e-01,  3.4917e-02,\n",
      "          -2.8986e-01, -1.2381e-01,  3.6250e-01,  3.2494e-01,  9.8000e-02,\n",
      "          -6.6426e-03, -1.0888e-01, -2.7173e-01, -3.3928e-01,  9.0723e-02,\n",
      "           1.8418e-01,  5.0582e-02, -1.2055e-01,  2.0626e-01, -1.0931e-02,\n",
      "          -2.4067e-01, -1.5237e-01, -5.2173e-02,  3.4522e-01,  1.4470e-01,\n",
      "          -1.2350e-01, -7.0452e-02, -3.9412e-01,  1.1495e-01,  5.3261e-01,\n",
      "           1.6105e-01,  4.8677e-02, -1.3728e-01, -4.0478e-02,  1.5165e-01,\n",
      "           1.9992e-01, -2.5297e-01, -2.2562e-01],\n",
      "         [ 6.9101e-02, -2.1846e-01,  1.3369e-01, -4.9444e-02,  1.6715e-01,\n",
      "           2.8393e-01,  2.2679e-01, -2.9722e-01,  2.4258e-01,  1.9142e-01,\n",
      "           1.0462e-01, -1.4125e-01, -2.5307e-01,  3.8116e-01, -5.4459e-02,\n",
      "           2.8649e-01, -3.2456e-01,  2.4105e-01, -2.1641e-01, -2.3955e-01,\n",
      "           1.7323e-01, -2.1895e-01, -3.2445e-01, -4.8006e-01,  1.4712e-01,\n",
      "           3.4940e-01,  5.0933e-01, -2.4338e-01, -5.7652e-02, -1.1761e-02,\n",
      "           2.3883e-01, -5.4607e-02,  1.7813e-01,  3.7565e-01, -3.4085e-02,\n",
      "          -2.0065e-01,  1.5212e-01, -1.4031e-01,  3.4623e-01, -2.5120e-02,\n",
      "           1.2741e-01, -6.1752e-02,  3.0878e-01,  1.3083e-01,  3.7138e-01,\n",
      "          -5.5579e-02,  1.2994e-02, -4.4959e-01,  1.5613e-01, -1.8079e-01,\n",
      "          -1.9671e-01, -1.3032e-01,  3.5197e-01,  3.8525e-01,  8.7489e-02,\n",
      "          -1.5742e-01, -4.3621e-02,  1.7523e-01, -3.7780e-01, -1.6787e-01,\n",
      "          -3.3497e-01, -4.2647e-01, -4.5050e-01,  1.9346e-01, -3.6656e-01,\n",
      "          -3.9095e-01,  4.7682e-03,  1.5169e-01, -3.3073e-01,  1.7162e-01,\n",
      "           1.3369e-01,  4.2995e-01, -3.0744e-01, -3.0849e-01,  5.3905e-02,\n",
      "           4.4929e-02, -1.1513e-01,  1.3015e-02, -9.6739e-03,  2.4341e-01,\n",
      "           3.4841e-01,  2.4598e-01,  6.8973e-02, -2.0587e-01,  2.2697e-01,\n",
      "           4.0024e-01, -9.2038e-02,  3.6523e-02, -1.4286e-01,  3.6884e-01,\n",
      "          -4.0241e-01,  4.7509e-01, -1.4440e-01,  7.4449e-02,  1.0598e-01,\n",
      "          -1.9055e-01, -3.2765e-01,  2.7219e-01,  2.4619e-01, -4.9501e-04,\n",
      "           6.1289e-02,  2.9555e-02, -2.5126e-01, -1.0201e-01,  1.2919e-01,\n",
      "           1.0291e-01, -1.1817e-01, -3.3204e-02,  1.2264e-01,  2.2225e-03,\n",
      "          -5.0340e-01, -1.9218e-01,  3.5726e-02,  3.6148e-01,  2.0753e-01,\n",
      "          -9.6278e-02, -5.6292e-02, -3.9057e-01,  8.5633e-02,  4.1709e-01,\n",
      "           1.9292e-01,  1.4890e-01, -7.5771e-02, -1.0695e-01,  1.5739e-01,\n",
      "           1.3537e-01, -3.2175e-01, -2.4750e-01]],\n",
      "\n",
      "        [[-2.1331e-01,  1.9870e-01,  4.2747e-01,  3.7002e-01, -1.8015e-01,\n",
      "           1.4479e-01, -3.7716e-01,  1.1508e-01, -4.1583e-01, -4.9053e-01,\n",
      "           2.5040e-01,  2.6824e-01, -1.5021e-01,  8.3107e-02, -1.5726e-02,\n",
      "           4.8377e-02,  2.8989e-01,  3.7175e-01, -5.5969e-02,  3.1935e-01,\n",
      "           7.5849e-02,  1.4834e-01,  2.2161e-01, -3.0629e-03, -7.0001e-02,\n",
      "           2.2173e-01, -1.2190e-01,  2.0312e-02, -1.3529e-01,  2.3964e-02,\n",
      "           1.2557e-01, -5.4378e-02, -1.8953e-01, -1.0969e-03, -1.0328e-01,\n",
      "           1.1161e-01,  1.0356e-01,  2.3028e-01,  7.0496e-02,  1.5581e-01,\n",
      "           1.5326e-02,  4.3574e-01, -3.8182e-01,  6.6456e-02,  2.4199e-01,\n",
      "          -2.1421e-01, -3.0467e-01,  1.1622e-01, -1.8367e-01, -1.4434e-01,\n",
      "          -3.4411e-01, -6.9080e-02, -3.5544e-02, -1.2473e-01, -1.0932e-02,\n",
      "           4.2548e-01,  2.7103e-01, -1.6970e-01,  1.0547e-01,  3.2401e-01,\n",
      "          -4.9586e-02,  1.1410e-01,  1.2913e-02,  3.5304e-01, -5.0215e-02,\n",
      "           2.1166e-01, -4.9258e-02,  7.2665e-02,  1.9668e-02,  2.2123e-02,\n",
      "          -2.7149e-01, -1.4019e-01,  1.3606e-02,  8.5012e-03,  4.4455e-01,\n",
      "           2.4961e-02, -2.2516e-01,  3.8911e-01,  1.0631e-01, -8.6187e-02,\n",
      "           3.1378e-01,  1.0800e-01,  4.1232e-01,  2.6003e-01,  8.6265e-02,\n",
      "          -5.9879e-02,  4.3661e-01, -1.0003e-01, -2.1758e-01,  1.0842e-01,\n",
      "           1.8513e-03,  4.2252e-03, -2.6209e-01, -1.0360e-01,  4.1178e-01,\n",
      "          -1.3997e-01, -1.3224e-01,  6.0028e-02,  4.1156e-01,  3.3367e-02,\n",
      "          -1.6979e-01,  2.4294e-01,  1.6355e-01,  9.7499e-02, -2.0043e-01,\n",
      "          -5.1673e-01,  4.6941e-01,  2.7052e-01, -7.6219e-02,  1.1453e-01,\n",
      "          -2.2307e-02,  5.4295e-02,  2.5524e-01, -1.9276e-01,  4.1217e-02,\n",
      "          -3.6951e-01, -2.2598e-01,  7.6875e-02, -7.4192e-02, -5.5125e-02,\n",
      "           7.9114e-02,  3.8219e-01, -1.9498e-01, -1.1813e-01,  3.4182e-01,\n",
      "          -4.1082e-01,  4.0259e-01, -1.4475e-01],\n",
      "         [-2.7099e-01,  5.4715e-01,  3.2521e-01,  4.5966e-01, -1.1112e-01,\n",
      "           3.7622e-01, -4.8489e-01,  1.7667e-01, -2.7844e-01, -2.9416e-01,\n",
      "           2.6203e-01,  3.4473e-02, -1.6417e-01, -7.0460e-02,  1.1014e-01,\n",
      "           6.2760e-02,  2.7803e-01,  3.6600e-01, -5.2252e-02,  3.8604e-01,\n",
      "           1.0027e-01,  2.1057e-01,  1.2173e-01,  8.9060e-02, -7.5294e-02,\n",
      "           3.2185e-01, -2.7169e-02,  1.6036e-02, -4.3379e-02,  3.1025e-02,\n",
      "           9.9505e-02,  9.1407e-02,  2.4055e-02,  4.5617e-02, -1.1436e-01,\n",
      "           5.3148e-02,  1.3466e-01,  2.4727e-01,  2.4024e-01,  8.6377e-02,\n",
      "          -4.1213e-02,  3.7716e-01, -5.3285e-01,  8.4727e-02,  2.6549e-01,\n",
      "          -2.6632e-01, -3.9978e-01, -1.3703e-03, -3.2366e-01, -2.2581e-01,\n",
      "          -5.6472e-01, -8.4483e-03, -7.5477e-02, -2.3586e-01, -4.8572e-02,\n",
      "           2.8875e-01,  3.1725e-01, -1.2073e-01,  9.0915e-03,  4.1182e-01,\n",
      "          -3.2191e-01,  1.7112e-01,  5.0571e-02,  4.2083e-01, -2.7664e-02,\n",
      "           9.5768e-02, -1.9716e-02,  1.4281e-01,  1.1358e-01,  9.8568e-03,\n",
      "          -3.3445e-01, -4.5472e-02, -3.6642e-02,  1.2086e-01,  2.5579e-01,\n",
      "           8.3628e-02, -1.7482e-01,  3.2272e-01, -1.2116e-01, -5.3270e-02,\n",
      "           2.6483e-01,  8.5493e-04,  3.1274e-01,  1.9269e-01,  1.1894e-01,\n",
      "           2.0334e-03,  2.7067e-01, -2.9806e-01, -1.1499e-01,  1.8787e-01,\n",
      "          -4.2089e-02,  5.4020e-02, -1.6258e-01, -1.2875e-01,  4.0621e-01,\n",
      "          -7.7317e-03, -1.7007e-01,  9.9990e-02,  4.0342e-01, -1.9598e-02,\n",
      "          -2.2353e-01,  2.4768e-01,  1.3346e-01,  1.2574e-01, -1.4097e-01,\n",
      "          -4.5026e-01,  4.4853e-01,  9.9196e-02,  1.2615e-01,  5.7220e-02,\n",
      "           1.5482e-01, -9.8447e-02,  1.9001e-01, -2.2765e-01,  1.7198e-01,\n",
      "          -3.5527e-01, -2.3754e-01,  2.3564e-02, -1.5237e-01,  5.6046e-02,\n",
      "           6.4004e-02,  3.2266e-01, -2.2660e-01,  1.4967e-03,  2.1200e-01,\n",
      "          -3.7083e-01,  4.4455e-01, -7.0950e-02]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]]])\n",
      "torch.Size([2, 32, 256])\n",
      "torch.Size([2, 32, 256])\n",
      "torch.Size([2, 2, 128])\n",
      "torch.Size([2, 2, 128])\n",
      "torch.Size([2, 2, 128])\n",
      "torch.Size([2, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "# we only take the initial_states of one layer\n",
    "def bidirectional_LSTM_forward(input, initial_states, initial_states_reverse, W_ih, W_hh, B_ih, B_hh, W_ih_reverse, W_hh_reverse, B_ih_reverse, B_hh_reverse):\n",
    "    output, (h_prev, c_prev) = LSTM_forward(input, initial_states, W_ih, W_hh, B_ih, B_hh)\n",
    "    output_reverse, (h_prev_reverse, c_prev_reverse) = LSTM_forward(torch.flip(input, [1]), initial_states_reverse, W_ih_reverse, W_hh_reverse, B_ih_reverse, B_hh_reverse)\n",
    "        \n",
    "    output_reverse = torch.flip(output_reverse, [1])\n",
    "    return torch.concatenate((output, output_reverse), dim=2), (torch.cat([h_prev, h_prev_reverse], dim=0), torch.cat([c_prev, c_prev_reverse], dim=0))\n",
    "        \n",
    "my_output, (my_h_n, my_c_n) = bidirectional_LSTM_forward(input, \\\n",
    "                                                         (h0[0], c0[0]), (h0[1], c0[1]), \\\n",
    "                                                         layer.weight_ih_l0, layer.weight_hh_l0, layer.bias_ih_l0, layer.bias_hh_l0,\\\n",
    "                                                         layer.weight_ih_l0_reverse, layer.weight_hh_l0_reverse, layer.bias_ih_l0_reverse, layer.bias_hh_l0_reverse)\n",
    "print(my_output)\n",
    "print(torch_output)\n",
    "print(my_h_n)\n",
    "print(torch_h_n)\n",
    "print(my_c_n)\n",
    "print(torch_c_n)\n",
    "\n",
    "print((my_c_n - torch_c_n)<0.000001)\n",
    "\n",
    "print(my_output.shape)\n",
    "print(torch_output.shape)\n",
    "print(my_h_n.shape)\n",
    "print(torch_h_n.shape)\n",
    "print(my_c_n.shape)\n",
    "print(torch_c_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48406d4",
   "metadata": {},
   "source": [
    "**Multilayers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1ec4e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "L = 32\n",
    "D = 1\n",
    "H_in = 64\n",
    "H_cell = 128\n",
    "H_out = H_cell\n",
    "num_layers = 3\n",
    "\n",
    "input = torch.rand(N, L, H_in) # N * L * H_in\n",
    "h0 = torch.rand(D*num_layers, N, H_out)\n",
    "c0 = torch.rand(D*num_layers, N, H_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cdd2a588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 128])\n",
      "torch.Size([3, 2, 128])\n",
      "torch.Size([3, 2, 128])\n",
      "weight_ih_l0\n",
      "weight_hh_l0\n",
      "bias_ih_l0\n",
      "bias_hh_l0\n",
      "weight_ih_l1\n",
      "weight_hh_l1\n",
      "bias_ih_l1\n",
      "bias_hh_l1\n",
      "weight_ih_l2\n",
      "weight_hh_l2\n",
      "bias_ih_l2\n",
      "bias_hh_l2\n"
     ]
    }
   ],
   "source": [
    "layer = nn.LSTM(H_in, H_cell, num_layers, batch_first=True, bidirectional=False)\n",
    "torch_output, (torch_h_n, torch_c_n) = layer(input, (h0, c0))\n",
    "print(torch_output.shape)\n",
    "print(torch_h_n.shape)\n",
    "print(torch_c_n.shape)\n",
    "for k, v in layer.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dbd57cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n",
      "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]]])\n",
      "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]],\n",
      "\n",
      "        [[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "# we only take the initial_states of one layer\n",
    "def LSTM_forward_multilayers(input, initial_states, W_ih, W_hh, B_ih, B_hh):\n",
    "    layers = len(W_ih)\n",
    "    \n",
    "    layer_outputs = []\n",
    "    layer_h_n = []\n",
    "    layer_c_n = []\n",
    "    prev_output = input\n",
    "    \n",
    "    for layer in range(0, layers):\n",
    "        prev_output, (h_n, c_n) = LSTM_forward(prev_output, (initial_states[0][layer], initial_states[1][layer]), W_ih[layer], W_hh[layer], B_ih[layer], B_hh[layer])\n",
    "        layer_outputs.append(prev_output)\n",
    "        layer_h_n.append(h_n)\n",
    "        layer_c_n.append(c_n)\n",
    "        \n",
    "    return prev_output, (torch.concatenate(layer_h_n, dim=0), torch.concatenate(layer_c_n, dim=0))\n",
    "\n",
    "\n",
    "\n",
    "W_ih = [layer.weight_ih_l0, layer.weight_ih_l1, layer.weight_ih_l2]\n",
    "W_hh = [layer.weight_hh_l0, layer.weight_hh_l1, layer.weight_hh_l2]\n",
    "B_ih = [layer.bias_ih_l0, layer.bias_ih_l1, layer.bias_ih_l2]\n",
    "B_hh = [layer.bias_hh_l0, layer.bias_hh_l1, layer.bias_hh_l2]\n",
    "\n",
    "my_output, (my_h_n, my_c_n) = LSTM_forward_multilayers(input, (h0, c0), W_ih, W_hh, B_ih, B_hh)\n",
    "# print(my_output)\n",
    "# print(torch_output)\n",
    "# print(my_h_n)\n",
    "# print(torch_h_n)\n",
    "# print(my_c_n)\n",
    "# print(torch_c_n)\n",
    "\n",
    "print((my_output - torch_output)<0.000001)\n",
    "print((my_h_n - torch_h_n)<0.000001)\n",
    "print((my_c_n - torch_c_n)<0.000001)\n",
    "\n",
    "# print(my_output.shape)\n",
    "# print(torch_output.shape)\n",
    "# print(my_h_n.shape)\n",
    "# print(torch_h_n.shape)\n",
    "# print(my_c_n.shape)\n",
    "# print(torch_c_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ae456",
   "metadata": {},
   "source": [
    "**Bidirectional Multilayer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d6a194c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "L = 32\n",
    "D = 2\n",
    "H_in = 64\n",
    "H_cell = 128\n",
    "H_out = H_cell\n",
    "num_layers = 3\n",
    "\n",
    "input = torch.rand(N, L, H_in) # N * L * H_in\n",
    "h0 = torch.rand(D*num_layers, N, H_out)\n",
    "c0 = torch.rand(D*num_layers, N, H_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c757aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 256])\n",
      "torch.Size([6, 2, 128])\n",
      "torch.Size([6, 2, 128])\n",
      "weight_ih_l0\n",
      "weight_hh_l0\n",
      "bias_ih_l0\n",
      "bias_hh_l0\n",
      "weight_ih_l0_reverse\n",
      "weight_hh_l0_reverse\n",
      "bias_ih_l0_reverse\n",
      "bias_hh_l0_reverse\n",
      "weight_ih_l1\n",
      "weight_hh_l1\n",
      "bias_ih_l1\n",
      "bias_hh_l1\n",
      "weight_ih_l1_reverse\n",
      "weight_hh_l1_reverse\n",
      "bias_ih_l1_reverse\n",
      "bias_hh_l1_reverse\n",
      "weight_ih_l2\n",
      "weight_hh_l2\n",
      "bias_ih_l2\n",
      "bias_hh_l2\n",
      "weight_ih_l2_reverse\n",
      "weight_hh_l2_reverse\n",
      "bias_ih_l2_reverse\n",
      "bias_hh_l2_reverse\n",
      "torch.Size([512, 64])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.LSTM(H_in, H_cell, num_layers, batch_first=True, bidirectional=True)\n",
    "torch_output, (torch_h_n, torch_c_n) = layer(input, (h0, c0))\n",
    "print(torch_output.shape)\n",
    "print(torch_h_n.shape)\n",
    "print(torch_c_n.shape)\n",
    "for k, v in layer.named_parameters():\n",
    "    print(k)\n",
    "    \n",
    "print(layer.weight_ih_l0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2721111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n",
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n",
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "# we only take the initial_states of one layer\n",
    "def bidirectional_LSTM_forward_multilayers(input, initial_states, initial_states_reverse, W_ih, W_hh, B_ih, B_hh, W_ih_reverse, W_hh_reverse, B_ih_reverse, B_hh_reverse):\n",
    "    layers = len(W_ih)\n",
    "    \n",
    "    layer_outputs = []\n",
    "    layer_h_n = []\n",
    "    layer_c_n = []\n",
    "    prev_output = input\n",
    "    \n",
    "    for layer in range(0, layers):\n",
    "        prev_output, (h_n, c_n) = bidirectional_LSTM_forward(prev_output, (initial_states[0][layer], initial_states[1][layer]), (initial_states_reverse[0][layer], initial_states_reverse[1][layer]), W_ih[layer], W_hh[layer], B_ih[layer], B_hh[layer], W_ih_reverse[layer], W_hh_reverse[layer], B_ih_reverse[layer], B_hh_reverse[layer])\n",
    "        layer_outputs.append(prev_output)\n",
    "        layer_h_n.append(h_n)\n",
    "        layer_c_n.append(c_n)\n",
    "        \n",
    "    return prev_output, (torch.concatenate(layer_h_n, dim=0), torch.concatenate(layer_c_n, dim=0))\n",
    "\n",
    "\n",
    "\n",
    "W_ih = [layer.weight_ih_l0, layer.weight_ih_l1, layer.weight_ih_l2]\n",
    "W_hh = [layer.weight_hh_l0, layer.weight_hh_l1, layer.weight_hh_l2]\n",
    "B_ih = [layer.bias_ih_l0, layer.bias_ih_l1, layer.bias_ih_l2]\n",
    "B_hh = [layer.bias_hh_l0, layer.bias_hh_l1, layer.bias_hh_l2]\n",
    "W_ih_reverse = [layer.weight_ih_l0_reverse, layer.weight_ih_l1_reverse, layer.weight_ih_l2_reverse]\n",
    "W_hh_reverse = [layer.weight_hh_l0_reverse, layer.weight_hh_l1_reverse, layer.weight_hh_l2_reverse]\n",
    "B_ih_reverse = [layer.bias_ih_l0_reverse, layer.bias_ih_l1_reverse, layer.bias_ih_l2_reverse]\n",
    "B_hh_reverse = [layer.bias_hh_l0_reverse, layer.bias_hh_l1_reverse, layer.bias_hh_l2_reverse]\n",
    "\n",
    "my_output, (my_h_n, my_c_n) = bidirectional_LSTM_forward_multilayers(input, (h0[0::2], c0[0::2]), (h0[1::2], c0[1::2]), W_ih, W_hh, B_ih, B_hh, W_ih_reverse, W_hh_reverse, B_ih_reverse, B_hh_reverse)\n",
    "# print(my_output)\n",
    "# print(torch_output)\n",
    "# print(my_h_n)\n",
    "# print(torch_h_n)\n",
    "# print(my_c_n)\n",
    "# print(torch_c_n)\n",
    "\n",
    "print((my_output - torch_output)<0.000001)\n",
    "print((my_h_n - torch_h_n)<0.000001)\n",
    "print((my_c_n - torch_c_n)<0.000001)\n",
    "\n",
    "# print(my_output.shape)\n",
    "# print(torch_output.shape)\n",
    "# print(my_h_n.shape)\n",
    "# print(torch_h_n.shape)\n",
    "# print(my_c_n.shape)\n",
    "# print(torch_c_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e253a941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for a in W_ih:\n",
    "    print(a.shape)\n",
    "    \n",
    "for a in B_ih:\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5414fdc4",
   "metadata": {},
   "source": [
    "**Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "639fd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, dims, hidden_dims, num_layers, bidirectional=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.W_ih = []\n",
    "        self.W_hh = []\n",
    "        self.B_ih = []\n",
    "        self.B_hh = []\n",
    "        \n",
    "        hidden_dims = 4 * hidden_dims\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.W_ih_reverse = []\n",
    "            self.W_hh_reverse = []\n",
    "            self.B_ih_reverse = []\n",
    "            self.B_hh_reverse = []\n",
    "            for layer in range(0, num_layers):\n",
    "                if layer == 0:\n",
    "                    self.W_ih.append(torch.rand(4 * hidden_dims, dims))\n",
    "                    self.W_ih_reverse.append(torch.rand(4 * hidden_dims, dims))\n",
    "                else:\n",
    "                    self.W_ih.append(torch.rand(4 * hidden_dims, 2 * hidden_dims))\n",
    "                    self.W_ih_reverse.append(torch.rand(4 * hidden_dims, 2 * hidden_dims))\n",
    "                self.W_hh.append(torch.rand(4 * hidden_dims, hidden_dims))\n",
    "                self.W_hh_reverse.append(torch.rand(4 * hidden_dims, hidden_dims))\n",
    "                self.B_ih.append(torch.rand(4 * hidden_dims, ))\n",
    "                self.B_ih_reverse.append(torch.rand(4 * hidden_dims, ))\n",
    "                self.B_hh.append(torch.rand(4 * hidden_dims, ))\n",
    "                self.B_hh_reverse.append(torch.rand(4 * hidden_dims, ))\n",
    "            self.W_ih_reverse = nn.ParameterList(self.W_ih_reverse)\n",
    "            self.W_hh_reverse = nn.ParameterList(self.W_hh_reverse)\n",
    "            self.B_ih_reverse = nn.ParameterList(self.B_ih_reverse)\n",
    "            self.B_hh_reverse = nn.ParameterList(self.B_hh_reverse)\n",
    "        else:\n",
    "            for layer in range(0, num_layers):\n",
    "                if layer == 0:\n",
    "                    self.W_ih.append(torch.rand(4 * hidden_dims, dims))\n",
    "                else:\n",
    "                    self.W_ih.append(torch.rand(4 * hidden_dims, hidden_dims))\n",
    "                self.W_hh.append(torch.rand(4 * hidden_dims, hidden_dims))\n",
    "                self.B_ih.append(torch.rand(4 * hidden_dims, ))\n",
    "                self.B_hh.append(torch.rand(4 * hidden_dims, ))\n",
    "                \n",
    "        self.W_ih = nn.ParameterList(self.W_ih)\n",
    "        self.W_hh = nn.ParameterList(self.W_hh)\n",
    "        self.B_ih = nn.ParameterList(self.B_ih)\n",
    "        self.B_hh = nn.ParameterList(self.B_hh)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        if self.bidirectional:\n",
    "            h0 = torch.zeros(2*len(self.W_ih), x.shape[0], self.W_ih[0].shape[0]//4)\n",
    "            c0 = torch.zeros(2*len(self.W_ih), x.shape[0], self.W_ih[0].shape[0]//4)\n",
    "            return bidirectional_LSTM_forward_multilayers(x, (h0[0::2], c0[0::2]), (h0[1::2], c0[1::2]), self.W_ih, self.W_hh, self.B_ih, self.B_hh, self.W_ih_reverse, self.W_hh_reverse, self.B_ih_reverse, self.B_hh_reverse)\n",
    "        else:\n",
    "            h0 = torch.zeros(len(self.W_ih), x.shape[0], self.W_ih[0].shape[0]//4)\n",
    "            c0 = torch.zeros(len(self.W_ih), x.shape[0], self.W_ih[0].shape[0]//4)\n",
    "\n",
    "            return LSTM_forward_multilayers(x, (h0, c0), self.W_ih, self.W_hh, self.B_ih, self.B_hh)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6889e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 2048])\n",
      "torch.Size([6, 2, 1024])\n",
      "torch.Size([6, 2, 1024])\n",
      "W_ih_reverse.0\n",
      "W_ih_reverse.1\n",
      "W_ih_reverse.2\n",
      "W_hh_reverse.0\n",
      "W_hh_reverse.1\n",
      "W_hh_reverse.2\n",
      "B_ih_reverse.0\n",
      "B_ih_reverse.1\n",
      "B_ih_reverse.2\n",
      "B_hh_reverse.0\n",
      "B_hh_reverse.1\n",
      "B_hh_reverse.2\n",
      "W_ih.0\n",
      "W_ih.1\n",
      "W_ih.2\n",
      "W_hh.0\n",
      "W_hh.1\n",
      "W_hh.2\n",
      "B_ih.0\n",
      "B_ih.1\n",
      "B_ih.2\n",
      "B_hh.0\n",
      "B_hh.1\n",
      "B_hh.2\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(2, 32, 128) # N * L * D\n",
    "layer = LSTM(128, 256, 3, bidirectional=True)\n",
    "my_output, (my_h_n, my_c_n) = layer(input)\n",
    "print(my_output.shape)\n",
    "print(my_h_n.shape)\n",
    "print(my_c_n.shape)\n",
    "\n",
    "for k, v in layer.named_parameters():\n",
    "    print(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
