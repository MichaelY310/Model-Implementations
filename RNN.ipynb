{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "02da60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "599d7671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 256])\n",
      "torch.Size([10, 1, 256])\n",
      "weight_ih_l0\n",
      "weight_hh_l0\n",
      "bias_ih_l0\n",
      "bias_hh_l0\n",
      "weight_ih_l1\n",
      "weight_hh_l1\n",
      "bias_ih_l1\n",
      "bias_hh_l1\n",
      "weight_ih_l2\n",
      "weight_hh_l2\n",
      "bias_ih_l2\n",
      "bias_hh_l2\n",
      "weight_ih_l3\n",
      "weight_hh_l3\n",
      "bias_ih_l3\n",
      "bias_hh_l3\n",
      "weight_ih_l4\n",
      "weight_hh_l4\n",
      "bias_ih_l4\n",
      "bias_hh_l4\n",
      "weight_ih_l5\n",
      "weight_hh_l5\n",
      "bias_ih_l5\n",
      "bias_hh_l5\n",
      "weight_ih_l6\n",
      "weight_hh_l6\n",
      "bias_ih_l6\n",
      "bias_hh_l6\n",
      "weight_ih_l7\n",
      "weight_hh_l7\n",
      "bias_ih_l7\n",
      "bias_hh_l7\n",
      "weight_ih_l8\n",
      "weight_hh_l8\n",
      "bias_ih_l8\n",
      "bias_hh_l8\n",
      "weight_ih_l9\n",
      "weight_hh_l9\n",
      "bias_ih_l9\n",
      "bias_hh_l9\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(1, 32, 128) # N * L * D\n",
    "layer = nn.RNN(128, 256, 10, batch_first=True)\n",
    "output = layer(input)\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)\n",
    "for k, v in layer.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4c3c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([20, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(1, 32, 128) # N * L * D\n",
    "layer = nn.RNN(128, 256, 10, batch_first=True, bidirectional=True)\n",
    "output = layer(input)\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e7741c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6322e-01,  2.4584e-02,  5.7583e-04,  ...,  4.0266e-02,\n",
      "           1.5913e-01,  4.9608e-01],\n",
      "         [ 4.3029e-02,  9.0179e-02, -6.4765e-02,  ...,  3.9127e-01,\n",
      "           2.9561e-01,  5.1282e-01],\n",
      "         [-9.0240e-02,  1.1956e-01, -6.9290e-02,  ...,  6.4814e-02,\n",
      "           3.1744e-01,  5.1586e-01],\n",
      "         ...,\n",
      "         [-1.0407e-01,  1.6431e-01, -3.0555e-02,  ...,  2.4383e-01,\n",
      "           1.5339e-01,  3.1201e-01],\n",
      "         [-7.1087e-02,  2.3048e-01,  1.4084e-01,  ...,  1.3087e-02,\n",
      "           1.8814e-01,  4.8251e-01],\n",
      "         [ 3.8113e-02, -1.9510e-01, -2.2524e-01,  ...,  7.2509e-02,\n",
      "           2.0123e-01,  4.0140e-01]],\n",
      "\n",
      "        [[ 9.8481e-02,  4.8401e-02, -1.3581e-01,  ...,  2.9438e-01,\n",
      "          -5.0514e-02,  2.4868e-01],\n",
      "         [ 4.4125e-04,  1.5117e-01,  1.6450e-01,  ...,  2.0371e-01,\n",
      "           2.7291e-01,  4.5843e-01],\n",
      "         [-2.2823e-01,  5.2925e-03,  6.3953e-03,  ...,  5.8448e-02,\n",
      "           3.5168e-02,  3.3025e-01],\n",
      "         ...,\n",
      "         [-1.8976e-01,  9.7708e-02, -5.0998e-02,  ...,  2.9952e-01,\n",
      "           2.1865e-01,  3.3558e-01],\n",
      "         [-1.7837e-02, -1.8817e-01,  2.5021e-02,  ...,  1.6805e-02,\n",
      "           9.8625e-02,  1.7617e-01],\n",
      "         [-2.0680e-01, -1.9865e-02, -2.9190e-02,  ..., -9.1643e-02,\n",
      "           1.6564e-01,  3.5913e-01]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 3.8113e-02, -1.9510e-01, -2.2524e-01, -5.5966e-01,  1.4392e-01,\n",
      "          -4.0054e-01, -4.7373e-01, -4.8263e-02,  3.1927e-01, -3.2806e-01,\n",
      "           9.1488e-02,  5.1779e-01, -3.0148e-01,  2.3234e-01, -5.4323e-02,\n",
      "           7.2428e-02,  2.5912e-01, -1.6317e-02,  1.7862e-01,  5.4737e-02,\n",
      "          -6.4064e-01,  1.9366e-01, -2.5445e-01, -3.3341e-01, -1.3181e-01,\n",
      "           1.2099e-01, -1.1657e-01,  1.3043e-01,  2.1945e-01, -3.3032e-01,\n",
      "          -3.8072e-01, -3.4338e-01,  1.8828e-01,  3.8195e-01,  2.4811e-02,\n",
      "           2.3372e-02,  1.1941e-03,  1.2126e-01,  5.5725e-01,  3.8357e-02,\n",
      "           2.3509e-01,  1.5673e-01,  3.3327e-01,  6.7877e-01, -4.4233e-02,\n",
      "          -2.6117e-01,  1.1379e-01, -2.3692e-02, -3.4763e-01,  5.4233e-02,\n",
      "          -4.8796e-02, -5.6057e-02, -5.6219e-02, -1.5573e-01,  4.4174e-02,\n",
      "          -3.0217e-01,  3.6000e-01, -1.5084e-01, -6.4837e-02,  1.7548e-02,\n",
      "          -2.5122e-01, -2.2338e-01, -3.3101e-01,  3.5089e-01, -6.5706e-02,\n",
      "           4.2824e-01, -3.7176e-01, -1.5278e-01, -5.9558e-02, -1.1535e-01,\n",
      "          -3.0791e-01, -1.6655e-02, -1.2751e-01,  2.1771e-01,  2.2395e-01,\n",
      "          -2.5693e-01,  2.6449e-01,  2.9725e-01, -1.7458e-01,  3.4735e-01,\n",
      "           6.1405e-02, -8.7609e-02, -1.6733e-01, -2.6268e-01, -1.2530e-01,\n",
      "          -1.6321e-02,  2.3029e-03,  4.0867e-01, -1.8897e-01,  3.2191e-01,\n",
      "           2.2309e-02, -3.2634e-01,  6.2916e-02,  3.3963e-01,  3.0480e-02,\n",
      "           1.9799e-01, -5.2180e-01, -1.4073e-01, -3.0504e-01,  3.0105e-01,\n",
      "           1.8436e-01, -3.3740e-03,  1.9279e-02,  1.2492e-02, -4.2284e-01,\n",
      "          -4.5478e-01,  3.2836e-01, -4.8776e-01,  2.9077e-01, -1.5636e-01,\n",
      "           3.9796e-01,  2.5500e-02, -2.9985e-01,  4.1920e-01,  1.8493e-01,\n",
      "          -3.7415e-01,  3.8039e-01,  3.2046e-02, -6.8382e-01,  1.1729e-01,\n",
      "           6.6691e-02, -4.0353e-01,  3.8128e-01, -4.0236e-01, -2.6578e-01,\n",
      "          -4.3760e-01,  1.1369e-01,  8.8171e-02,  3.2536e-01,  4.4956e-01,\n",
      "           7.3571e-02, -1.1700e-01,  1.0544e-01,  9.6771e-02,  3.7711e-02,\n",
      "           5.9458e-01, -3.9655e-01,  6.2574e-01, -1.1840e-02, -5.3302e-01,\n",
      "           2.0419e-01, -4.5634e-01,  1.3638e-01, -1.0875e-01, -3.5639e-01,\n",
      "          -4.3485e-01, -2.4464e-02,  3.3323e-01, -3.7672e-01, -3.0838e-01,\n",
      "          -5.4621e-01,  1.4233e-01,  4.9360e-01,  2.5862e-01, -4.3468e-01,\n",
      "           4.4026e-01,  4.8868e-01,  9.4018e-03, -2.7969e-01,  4.1383e-01,\n",
      "          -2.9355e-02,  1.7431e-01,  1.7315e-01,  4.8755e-01,  1.8915e-01,\n",
      "          -1.8166e-01, -4.0918e-01,  1.4399e-01, -1.0494e-02,  2.1354e-01,\n",
      "           2.7979e-01,  8.8576e-02,  1.9771e-01,  6.4034e-02, -1.0065e-01,\n",
      "          -9.5844e-02,  4.2564e-01,  1.3764e-01,  1.0724e-01,  9.6438e-02,\n",
      "          -1.2450e-01, -4.2507e-01,  3.1011e-01,  1.2324e-01,  1.6454e-01,\n",
      "           2.7824e-01, -3.9620e-02,  6.7852e-02, -6.6761e-02, -3.3380e-01,\n",
      "           4.0214e-01, -2.5551e-02, -1.4436e-01,  4.5104e-01,  1.7671e-01,\n",
      "          -1.9459e-01,  2.3305e-01, -3.6105e-01,  2.4543e-01,  7.6930e-02,\n",
      "          -3.4501e-01,  1.1135e-01, -3.7827e-01, -1.5808e-01, -5.4973e-01,\n",
      "           1.3461e-01, -1.2286e-01,  5.3108e-01, -1.4866e-01,  5.0120e-02,\n",
      "           1.3959e-01,  2.3660e-01,  1.6405e-01,  1.2659e-01,  6.1369e-02,\n",
      "           4.3614e-01, -4.0430e-02, -2.6990e-01,  6.0402e-01,  2.9599e-03,\n",
      "          -2.9269e-01, -4.2153e-01,  1.1945e-01,  9.3604e-02, -2.4298e-01,\n",
      "           2.8713e-01, -4.7138e-01, -3.8899e-01, -1.4366e-02,  4.7124e-01,\n",
      "           4.5258e-01, -2.1340e-01, -5.7199e-02,  5.2881e-01,  2.1363e-02,\n",
      "           2.6448e-01, -3.0759e-01, -1.0052e-01, -1.4483e-01, -6.0098e-01,\n",
      "           1.2807e-01,  1.4871e-01, -1.3752e-02, -8.2202e-02,  1.3320e-01,\n",
      "           1.9000e-03, -2.3096e-01, -3.2079e-01, -4.1185e-01, -3.7299e-02,\n",
      "          -1.5789e-02,  3.5078e-02,  2.1440e-01,  7.2509e-02,  2.0123e-01,\n",
      "           4.0140e-01],\n",
      "         [-2.0680e-01, -1.9865e-02, -2.9190e-02, -4.2225e-01,  3.2327e-01,\n",
      "          -1.7700e-01, -2.5238e-01, -3.2890e-04,  3.3772e-01, -1.5128e-01,\n",
      "           8.1745e-02,  1.8436e-01, -1.1763e-01,  3.8435e-01, -1.6000e-01,\n",
      "           3.0260e-01,  1.2174e-01,  1.6050e-01,  5.2235e-02,  1.3886e-01,\n",
      "          -5.1683e-01, -2.6383e-01, -2.0712e-01, -2.1834e-01, -8.8793e-02,\n",
      "           2.9733e-01, -1.1939e-01, -4.9999e-02,  1.0495e-02, -4.9087e-01,\n",
      "          -2.5971e-01, -1.2394e-01, -1.7231e-01,  3.1872e-01, -1.2995e-02,\n",
      "           1.6757e-01, -2.6786e-01, -2.5258e-02,  3.7062e-01,  1.2792e-01,\n",
      "          -2.0876e-01,  3.7866e-01,  3.3705e-01,  4.2262e-01,  2.7553e-01,\n",
      "          -1.3093e-01,  1.8704e-01, -1.5947e-01, -8.2697e-02,  2.2815e-01,\n",
      "           2.0287e-01,  1.7301e-01, -5.5825e-02, -5.0838e-02,  3.8417e-01,\n",
      "          -2.0732e-01,  3.9900e-01, -4.3178e-03, -1.1734e-01, -1.9769e-01,\n",
      "          -3.6259e-01, -1.6621e-01, -3.6009e-01,  3.2318e-01, -1.7979e-02,\n",
      "           4.9580e-01, -2.2349e-01,  2.5820e-01, -1.2974e-01, -2.0471e-01,\n",
      "          -3.2526e-01, -1.8741e-01, -2.5106e-01,  2.8792e-01,  1.3952e-01,\n",
      "           7.8158e-02,  2.7088e-01, -1.0715e-01, -3.0971e-01,  1.3231e-01,\n",
      "          -4.1513e-01,  2.1481e-02, -9.7820e-02,  6.4451e-03, -1.6469e-01,\n",
      "           2.0642e-01,  3.7838e-01,  3.3332e-01, -1.1589e-01,  1.7274e-01,\n",
      "          -1.3512e-01, -2.9910e-01,  6.7391e-02,  2.3350e-01, -1.0232e-01,\n",
      "           9.8603e-03, -4.6314e-01, -1.4155e-01, -4.7433e-01,  3.6786e-01,\n",
      "          -1.0409e-01, -1.1540e-01, -1.6576e-01,  2.0889e-01, -4.5538e-01,\n",
      "          -2.3505e-01,  1.4140e-01, -2.9022e-01, -2.1627e-02,  1.2655e-02,\n",
      "          -8.4405e-02, -1.2133e-01,  5.9055e-02,  5.6757e-01,  2.8317e-02,\n",
      "          -3.8018e-01,  1.4097e-01, -1.6796e-01, -7.8061e-01, -3.2080e-01,\n",
      "          -3.4762e-02, -4.5093e-01,  3.9641e-01,  2.3435e-01, -2.6627e-01,\n",
      "          -4.2813e-01,  2.5414e-01, -2.4554e-01, -6.5318e-02,  4.4091e-01,\n",
      "           1.9211e-01, -2.8428e-01, -5.4821e-02,  1.6697e-01, -3.4864e-02,\n",
      "           4.3407e-01, -4.5503e-01,  6.3714e-01, -2.2210e-01, -2.4394e-01,\n",
      "           1.2783e-01, -2.9080e-01,  9.5474e-03, -8.5020e-03, -2.2521e-01,\n",
      "          -4.4733e-01, -1.0469e-01,  2.1608e-01, -2.1964e-01,  1.5771e-02,\n",
      "          -2.2810e-01,  3.5876e-01,  6.0685e-01,  2.7830e-01, -1.6184e-01,\n",
      "           3.4079e-01,  3.2419e-01,  2.3676e-01, -6.0375e-02,  2.6954e-01,\n",
      "           1.5218e-01, -2.4497e-01,  2.0066e-01,  3.4747e-01,  7.4942e-02,\n",
      "          -4.1197e-01, -3.2600e-01, -9.7357e-02,  2.8461e-01,  3.2600e-01,\n",
      "           2.7109e-01,  5.9923e-02,  2.7768e-01,  1.3034e-01, -2.8311e-01,\n",
      "           2.4528e-01,  6.0386e-01,  6.6319e-02,  1.3235e-01, -1.5533e-01,\n",
      "          -3.5786e-01, -3.4760e-01,  2.4890e-01, -3.3153e-02,  6.9258e-02,\n",
      "           3.5763e-01, -1.0589e-01,  3.7694e-01, -2.2906e-01, -5.5238e-01,\n",
      "           2.8242e-01, -1.7737e-01,  3.7293e-02,  3.3048e-01,  1.5766e-01,\n",
      "          -3.9532e-01,  1.1534e-01, -3.2173e-01,  1.9558e-01,  3.4285e-01,\n",
      "          -4.4324e-01, -1.4349e-01,  2.7819e-02, -3.6147e-01, -6.8124e-01,\n",
      "           1.3582e-01, -2.0644e-02,  5.1655e-02,  1.5422e-01, -7.6019e-02,\n",
      "          -2.1604e-03,  2.2637e-01,  4.7518e-02,  8.7986e-02,  6.3158e-02,\n",
      "           4.4220e-01, -9.8670e-02, -3.0969e-01,  7.1066e-01, -1.4121e-01,\n",
      "          -3.5346e-01, -4.1605e-01,  1.4903e-01,  5.6651e-02, -2.3231e-01,\n",
      "           2.0997e-01, -4.0560e-01,  1.4469e-01, -6.0746e-03,  1.7865e-01,\n",
      "           4.3145e-01, -7.3943e-02,  1.7391e-01,  5.5672e-01,  1.6986e-01,\n",
      "          -2.4824e-01, -4.6284e-01,  1.1332e-01,  4.0757e-03, -5.7233e-01,\n",
      "          -1.0065e-01, -9.2525e-02, -1.4543e-01, -3.6295e-01, -4.3233e-03,\n",
      "          -1.7655e-01, -2.6858e-01, -7.7438e-02, -3.7358e-01,  9.0869e-02,\n",
      "          -2.4800e-01, -2.0997e-02,  4.4661e-01, -9.1643e-02,  1.6564e-01,\n",
      "           3.5913e-01]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 1.6322e-01,  2.4584e-02,  5.7583e-04,  ...,  4.0266e-02,\n",
      "           1.5913e-01,  4.9608e-01],\n",
      "         [ 4.3029e-02,  9.0180e-02, -6.4765e-02,  ...,  3.9127e-01,\n",
      "           2.9561e-01,  5.1282e-01],\n",
      "         [-9.0240e-02,  1.1956e-01, -6.9290e-02,  ...,  6.4814e-02,\n",
      "           3.1744e-01,  5.1586e-01],\n",
      "         ...,\n",
      "         [-1.0407e-01,  1.6431e-01, -3.0555e-02,  ...,  2.4383e-01,\n",
      "           1.5339e-01,  3.1201e-01],\n",
      "         [-7.1087e-02,  2.3048e-01,  1.4084e-01,  ...,  1.3087e-02,\n",
      "           1.8814e-01,  4.8251e-01],\n",
      "         [ 3.8113e-02, -1.9510e-01, -2.2524e-01,  ...,  7.2509e-02,\n",
      "           2.0123e-01,  4.0140e-01]],\n",
      "\n",
      "        [[ 9.8481e-02,  4.8401e-02, -1.3581e-01,  ...,  2.9438e-01,\n",
      "          -5.0514e-02,  2.4868e-01],\n",
      "         [ 4.4122e-04,  1.5117e-01,  1.6450e-01,  ...,  2.0371e-01,\n",
      "           2.7291e-01,  4.5843e-01],\n",
      "         [-2.2823e-01,  5.2924e-03,  6.3954e-03,  ...,  5.8448e-02,\n",
      "           3.5168e-02,  3.3025e-01],\n",
      "         ...,\n",
      "         [-1.8976e-01,  9.7707e-02, -5.0998e-02,  ...,  2.9952e-01,\n",
      "           2.1865e-01,  3.3558e-01],\n",
      "         [-1.7837e-02, -1.8817e-01,  2.5021e-02,  ...,  1.6805e-02,\n",
      "           9.8625e-02,  1.7617e-01],\n",
      "         [-2.0680e-01, -1.9865e-02, -2.9190e-02,  ..., -9.1643e-02,\n",
      "           1.6564e-01,  3.5913e-01]]], grad_fn=<CopySlices>)\n",
      "torch.Size([1, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# one layer\n",
    "\n",
    "def rnn_forward(input, W_ih, W_hh, B_ih, B_hh, h_0):\n",
    "    L = input.shape[1]\n",
    "    output = torch.zeros(input.shape[0], input.shape[1], W_ih.shape[0])\n",
    "    h_prev = h_0\n",
    "    \n",
    "    for t in range(0, L):\n",
    "        x_t = input[:, t, :]\n",
    "        h_prev = torch.tanh(x_t @ W_ih.T + B_ih + h_prev @ W_hh.T + B_hh)\n",
    "        output[:, t, :] = h_prev\n",
    "        \n",
    "    return output, h_prev\n",
    "\n",
    "input = torch.rand(2, 32, 128) # N * L * D\n",
    "h_0 = torch.zeros(1, 1, 256)\n",
    "layer = nn.RNN(128, 256, 1, batch_first=True)\n",
    "output, h_n = layer(input)\n",
    "print(output)\n",
    "print(h_n)\n",
    "output, h_n = rnn_forward(input, layer.weight_ih_l0, layer.weight_hh_l0, layer.bias_ih_l0, layer.bias_hh_l0, h_0)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9af94",
   "metadata": {},
   "source": [
    "**Outputs are concatenated along the sequence length dimension. The reversed part should be reversed back**\n",
    "**Hidden states are concatenated along the batch dimension. It is not reversed back to retain information it learnt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "55a9e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0568,  0.1580,  0.0554,  ...,  0.0472, -0.3300, -0.3740],\n",
      "         [ 0.1640,  0.1496, -0.1464,  ..., -0.1184, -0.2303, -0.2452],\n",
      "         [-0.1713,  0.0829, -0.1965,  ...,  0.0542, -0.3465, -0.2295],\n",
      "         ...,\n",
      "         [ 0.0247, -0.0193, -0.3233,  ...,  0.0061, -0.3055, -0.2373],\n",
      "         [ 0.0373,  0.0874,  0.0355,  ...,  0.0684, -0.2353, -0.3872],\n",
      "         [-0.1355,  0.1523, -0.2585,  ...,  0.2241, -0.1634, -0.6356]],\n",
      "\n",
      "        [[ 0.2082, -0.0737, -0.0546,  ...,  0.0469,  0.0127, -0.2052],\n",
      "         [-0.0714, -0.1095, -0.2451,  ..., -0.0404, -0.4353, -0.2885],\n",
      "         [ 0.2210, -0.0547, -0.1427,  ..., -0.1025, -0.2274, -0.4809],\n",
      "         ...,\n",
      "         [-0.0228,  0.0934, -0.0999,  ..., -0.1799, -0.3475, -0.3763],\n",
      "         [-0.0653,  0.1755, -0.1629,  ..., -0.0107, -0.5088, -0.5674],\n",
      "         [ 0.2802, -0.2470, -0.0702,  ...,  0.0027, -0.0331, -0.3965]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.1355,  0.1523, -0.2585,  ..., -0.0304,  0.1876,  0.0636],\n",
      "         [ 0.2802, -0.2470, -0.0702,  ..., -0.0230,  0.1562,  0.0828]],\n",
      "\n",
      "        [[-0.3805, -0.5366,  0.2129,  ...,  0.0472, -0.3300, -0.3740],\n",
      "         [-0.2314, -0.3484, -0.1305,  ...,  0.0469,  0.0127, -0.2052]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.0568,  0.1580,  0.0554,  ...,  0.0472, -0.3300, -0.3740],\n",
      "         [ 0.1640,  0.1496, -0.1464,  ..., -0.1184, -0.2303, -0.2452],\n",
      "         [-0.1713,  0.0829, -0.1965,  ...,  0.0542, -0.3465, -0.2295],\n",
      "         ...,\n",
      "         [ 0.0247, -0.0193, -0.3233,  ...,  0.0061, -0.3055, -0.2373],\n",
      "         [ 0.0373,  0.0874,  0.0355,  ...,  0.0684, -0.2353, -0.3872],\n",
      "         [-0.1355,  0.1523, -0.2585,  ...,  0.2241, -0.1634, -0.6356]],\n",
      "\n",
      "        [[ 0.2082, -0.0737, -0.0546,  ...,  0.0469,  0.0127, -0.2052],\n",
      "         [-0.0714, -0.1095, -0.2451,  ..., -0.0404, -0.4353, -0.2885],\n",
      "         [ 0.2210, -0.0547, -0.1427,  ..., -0.1025, -0.2274, -0.4809],\n",
      "         ...,\n",
      "         [-0.0228,  0.0934, -0.0999,  ..., -0.1799, -0.3475, -0.3763],\n",
      "         [-0.0653,  0.1755, -0.1629,  ..., -0.0107, -0.5088, -0.5674],\n",
      "         [ 0.2802, -0.2470, -0.0702,  ...,  0.0027, -0.0331, -0.3965]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[-0.1355,  0.1523, -0.2585,  ..., -0.0304,  0.1876,  0.0636],\n",
      "         [ 0.2802, -0.2470, -0.0702,  ..., -0.0230,  0.1562,  0.0828]],\n",
      "\n",
      "        [[-0.3805, -0.5366,  0.2129,  ...,  0.0472, -0.3300, -0.3740],\n",
      "         [-0.2314, -0.3484, -0.1305,  ...,  0.0469,  0.0127, -0.2052]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# one layer\n",
    "\n",
    "def bidirectional_rnn_forward(input, W_ih, W_hh, B_ih, B_hh, h_0, W_reverse_ih, W_reverse_hh, B_reverse_ih, B_reverse_hh, h_reverse_0):\n",
    "    L = input.shape[1]\n",
    "    N = input.shape[0]\n",
    "    hidden_D = W_ih.shape[0]\n",
    "    \n",
    "    combined_output = torch.zeros(N, L, 2 * hidden_D)\n",
    "    \n",
    "    output, h_prev = rnn_forward(input, W_ih, W_hh, B_ih, B_hh, h_0)\n",
    "    output_reverse, h_prev_reverse = rnn_forward(torch.flip(input, [1]), W_reverse_ih, W_reverse_hh, B_reverse_ih, B_reverse_hh, h_reverse_0)\n",
    "    # output is reversed back, hidden is not \n",
    "    output_reverse = torch.flip(output_reverse, [1])\n",
    "        \n",
    "    combined_output[:, :, :hidden_D] = output\n",
    "    combined_output[:, :, hidden_D:] = output_reverse\n",
    "    return combined_output, torch.cat([h_prev, h_prev_reverse], dim=0)\n",
    "\n",
    "input = torch.rand(2, 32, 128) # N * L * D\n",
    "h_0 = torch.zeros(1, 1, 256)\n",
    "h_0_reverse = torch.zeros(1, 1, 256)\n",
    "layer = nn.RNN(128, 256, 1, batch_first=True, bidirectional=True)\n",
    "output, h_n = layer(input)\n",
    "print(output)\n",
    "print(h_n)\n",
    "output, h_n = bidirectional_rnn_forward(input, layer.weight_ih_l0, layer.weight_hh_l0, layer.bias_ih_l0, layer.bias_hh_l0, h_0, layer.weight_ih_l0_reverse, layer.weight_hh_l0_reverse, layer.bias_ih_l0_reverse, layer.bias_hh_l0_reverse, h_0_reverse)\n",
    "print(output)\n",
    "print(h_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871217d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "337a0e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0260,  0.0707, -0.0743,  ..., -0.1762, -0.2624,  0.0388],\n",
      "         [-0.1117,  0.3146, -0.0922,  ..., -0.1021, -0.1647,  0.0743],\n",
      "         [-0.0768,  0.3284,  0.0390,  ..., -0.2411, -0.1844,  0.1068],\n",
      "         ...,\n",
      "         [-0.0696,  0.1940,  0.0810,  ..., -0.0991, -0.1461,  0.0283],\n",
      "         [-0.1369,  0.2195,  0.0966,  ..., -0.2227, -0.1920,  0.0370],\n",
      "         [-0.0826,  0.2015,  0.1103,  ..., -0.1661, -0.2656,  0.0510]],\n",
      "\n",
      "        [[ 0.0208,  0.0494, -0.0766,  ..., -0.2300, -0.2538,  0.0535],\n",
      "         [-0.0821,  0.2166, -0.0123,  ..., -0.2222, -0.2386,  0.0378],\n",
      "         [-0.0468,  0.2966,  0.1014,  ..., -0.1403, -0.1236, -0.0315],\n",
      "         ...,\n",
      "         [ 0.0252,  0.3453,  0.1335,  ..., -0.0645, -0.1282,  0.0203],\n",
      "         [-0.0182,  0.3488,  0.1221,  ..., -0.0972, -0.2039,  0.1223],\n",
      "         [ 0.0043,  0.3280,  0.0614,  ..., -0.2298, -0.1836, -0.0687]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.0260,  0.0707, -0.0743,  ..., -0.1762, -0.2624,  0.0388],\n",
      "         [-0.1117,  0.3146, -0.0922,  ..., -0.1021, -0.1647,  0.0743],\n",
      "         [-0.0768,  0.3284,  0.0390,  ..., -0.2411, -0.1844,  0.1068],\n",
      "         ...,\n",
      "         [-0.0696,  0.1940,  0.0810,  ..., -0.0991, -0.1461,  0.0283],\n",
      "         [-0.1369,  0.2195,  0.0966,  ..., -0.2227, -0.1920,  0.0370],\n",
      "         [-0.0826,  0.2015,  0.1103,  ..., -0.1661, -0.2656,  0.0510]],\n",
      "\n",
      "        [[ 0.0208,  0.0494, -0.0766,  ..., -0.2300, -0.2538,  0.0535],\n",
      "         [-0.0821,  0.2166, -0.0123,  ..., -0.2222, -0.2386,  0.0378],\n",
      "         [-0.0468,  0.2966,  0.1014,  ..., -0.1403, -0.1236, -0.0315],\n",
      "         ...,\n",
      "         [ 0.0252,  0.3453,  0.1335,  ..., -0.0645, -0.1282,  0.0203],\n",
      "         [-0.0182,  0.3488,  0.1221,  ..., -0.0972, -0.2039,  0.1223],\n",
      "         [ 0.0043,  0.3280,  0.0614,  ..., -0.2298, -0.1836, -0.0687]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.2227,  0.1732,  0.1336,  ..., -0.0310,  0.0191, -0.1256],\n",
      "         [-0.1242,  0.2824,  0.0493,  ...,  0.2095,  0.0507,  0.1605]],\n",
      "\n",
      "        [[-0.1641,  0.2111,  0.0765,  ...,  0.1299,  0.1364,  0.1122],\n",
      "         [-0.1887,  0.2993, -0.1457,  ...,  0.0596,  0.0904,  0.0613]],\n",
      "\n",
      "        [[-0.0826,  0.2015,  0.1103,  ..., -0.1661, -0.2656,  0.0510],\n",
      "         [ 0.0043,  0.3280,  0.0614,  ..., -0.2298, -0.1836, -0.0687]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.2227,  0.1732,  0.1336,  ..., -0.0310,  0.0191, -0.1256],\n",
      "         [-0.1242,  0.2824,  0.0493,  ...,  0.2095,  0.0507,  0.1605]],\n",
      "\n",
      "        [[-0.1641,  0.2111,  0.0765,  ...,  0.1299,  0.1364,  0.1122],\n",
      "         [-0.1887,  0.2993, -0.1457,  ...,  0.0596,  0.0904,  0.0613]],\n",
      "\n",
      "        [[-0.0826,  0.2015,  0.1103,  ..., -0.1661, -0.2656,  0.0510],\n",
      "         [ 0.0043,  0.3280,  0.0614,  ..., -0.2298, -0.1836, -0.0687]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# multilayers\n",
    "# the last hidden state of the pervious layer doesn't affect the next layer.\n",
    "\n",
    "def rnn_forward_multilayers(input, W_ih, W_hh, B_ih, B_hh):\n",
    "    h_0 = torch.zeros(1, 1, W_ih[0].shape[0])\n",
    "    h_n = []\n",
    "    \n",
    "    for layer in range(len(W_ih)):\n",
    "        output, h_prev = rnn_forward(input, W_ih[layer], W_hh[layer], B_ih[layer], B_hh[layer], h_0)\n",
    "        input = output\n",
    "        h_n.append(h_prev)\n",
    "    return output, torch.cat(h_n, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "input = torch.rand(2, 32, 128) # N * L * D\n",
    "layer = nn.RNN(128, 256, 3, batch_first=True)\n",
    "torch_output, torch_h_n = layer(input)\n",
    "\n",
    "\n",
    "W_ih = [layer.weight_ih_l0, layer.weight_ih_l1, layer.weight_ih_l2]\n",
    "W_hh = [layer.weight_hh_l0, layer.weight_hh_l1, layer.weight_hh_l2] \n",
    "B_ih = [layer.bias_ih_l0, layer.bias_ih_l1, layer.bias_ih_l2]\n",
    "B_hh = [layer.bias_hh_l0, layer.bias_hh_l1, layer.bias_hh_l2]\n",
    "my_output, my_h_n = rnn_forward_multilayers(input, W_ih, W_hh, B_ih, B_hh)\n",
    "\n",
    "print(torch_output)\n",
    "print(my_output)\n",
    "\n",
    "print(torch_h_n)\n",
    "print(my_h_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0ca17fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.2871e-02,  8.2766e-02,  9.8542e-02,  ...,  3.1438e-01,\n",
      "          -1.8367e-01,  3.2801e-01],\n",
      "         [-2.1307e-01,  2.2806e-01,  4.6400e-03,  ...,  1.4595e-01,\n",
      "          -5.3105e-02,  1.0537e-01],\n",
      "         [-2.6231e-01,  3.4964e-01,  5.0635e-02,  ...,  1.2524e-01,\n",
      "          -3.6468e-01,  4.3890e-02],\n",
      "         ...,\n",
      "         [-3.0108e-01,  4.6312e-01, -2.1220e-01,  ...,  1.8264e-01,\n",
      "           1.1459e-02,  2.7888e-01],\n",
      "         [-3.3551e-01,  3.8755e-01, -2.7315e-02,  ...,  9.4507e-02,\n",
      "          -7.3615e-04,  3.2940e-01],\n",
      "         [-1.8453e-01,  4.9384e-01, -6.3438e-02,  ...,  8.1001e-02,\n",
      "          -1.0529e-01,  1.8432e-01]],\n",
      "\n",
      "        [[ 2.4146e-04,  1.4694e-01, -1.5009e-01,  ...,  3.8884e-02,\n",
      "          -2.2971e-02,  2.8263e-01],\n",
      "         [-1.0530e-01,  1.9030e-01,  8.2592e-02,  ...,  3.6136e-01,\n",
      "          -8.9375e-02,  2.4751e-01],\n",
      "         [-3.6790e-01,  2.6136e-01,  2.8915e-02,  ...,  7.5365e-02,\n",
      "          -3.1295e-01,  4.0544e-01],\n",
      "         ...,\n",
      "         [-2.9997e-01,  3.1493e-01, -1.0654e-02,  ...,  1.8241e-01,\n",
      "          -2.1700e-01,  1.7796e-01],\n",
      "         [-4.4233e-01,  4.5361e-01, -3.3551e-02,  ...,  1.1371e-02,\n",
      "           4.5080e-02,  4.0638e-01],\n",
      "         [-4.0569e-01,  4.4376e-01,  6.6162e-02,  ...,  2.7825e-02,\n",
      "          -8.9011e-03,  2.7178e-01]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-2.2871e-02,  8.2766e-02,  9.8542e-02,  ...,  3.1438e-01,\n",
      "          -1.8367e-01,  3.2801e-01],\n",
      "         [-2.1307e-01,  2.2806e-01,  4.6399e-03,  ...,  1.4595e-01,\n",
      "          -5.3104e-02,  1.0537e-01],\n",
      "         [-2.6231e-01,  3.4964e-01,  5.0635e-02,  ...,  1.2524e-01,\n",
      "          -3.6468e-01,  4.3890e-02],\n",
      "         ...,\n",
      "         [-3.0108e-01,  4.6312e-01, -2.1220e-01,  ...,  1.8264e-01,\n",
      "           1.1459e-02,  2.7888e-01],\n",
      "         [-3.3551e-01,  3.8755e-01, -2.7315e-02,  ...,  9.4507e-02,\n",
      "          -7.3621e-04,  3.2940e-01],\n",
      "         [-1.8453e-01,  4.9384e-01, -6.3438e-02,  ...,  8.1001e-02,\n",
      "          -1.0529e-01,  1.8432e-01]],\n",
      "\n",
      "        [[ 2.4145e-04,  1.4694e-01, -1.5009e-01,  ...,  3.8884e-02,\n",
      "          -2.2971e-02,  2.8263e-01],\n",
      "         [-1.0530e-01,  1.9030e-01,  8.2592e-02,  ...,  3.6136e-01,\n",
      "          -8.9375e-02,  2.4751e-01],\n",
      "         [-3.6790e-01,  2.6136e-01,  2.8915e-02,  ...,  7.5365e-02,\n",
      "          -3.1295e-01,  4.0544e-01],\n",
      "         ...,\n",
      "         [-2.9997e-01,  3.1493e-01, -1.0653e-02,  ...,  1.8241e-01,\n",
      "          -2.1700e-01,  1.7796e-01],\n",
      "         [-4.4233e-01,  4.5361e-01, -3.3551e-02,  ...,  1.1371e-02,\n",
      "           4.5080e-02,  4.0638e-01],\n",
      "         [-4.0569e-01,  4.4376e-01,  6.6162e-02,  ...,  2.7825e-02,\n",
      "          -8.9011e-03,  2.7178e-01]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.0972, -0.2197,  0.2254,  ...,  0.0952,  0.2111, -0.0676],\n",
      "         [ 0.1341, -0.1691,  0.1966,  ...,  0.1980,  0.0049, -0.3242]],\n",
      "\n",
      "        [[ 0.5748,  0.0660,  0.4087,  ...,  0.4005, -0.1963, -0.0752],\n",
      "         [ 0.4575,  0.3128,  0.1239,  ...,  0.1246, -0.2736, -0.2763]],\n",
      "\n",
      "        [[ 0.2520,  0.2381, -0.1136,  ...,  0.0835,  0.4746, -0.1767],\n",
      "         [ 0.1162, -0.1164, -0.1585,  ...,  0.2531,  0.5253, -0.2783]],\n",
      "\n",
      "        [[-0.3191,  0.2705, -0.3290,  ..., -0.3201,  0.2127,  0.0008],\n",
      "         [-0.2333,  0.2911, -0.3106,  ..., -0.3824,  0.1873, -0.0090]],\n",
      "\n",
      "        [[-0.1845,  0.4938, -0.0634,  ...,  0.2029,  0.4314,  0.5049],\n",
      "         [-0.4057,  0.4438,  0.0662,  ...,  0.1689,  0.2410,  0.5475]],\n",
      "\n",
      "        [[-0.0851,  0.0390, -0.2925,  ...,  0.3144, -0.1837,  0.3280],\n",
      "         [-0.0820, -0.2449, -0.1946,  ...,  0.0389, -0.0230,  0.2826]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.0972, -0.2197,  0.2254,  ...,  0.0952,  0.2111, -0.0676],\n",
      "         [ 0.1341, -0.1691,  0.1966,  ...,  0.1980,  0.0049, -0.3242]],\n",
      "\n",
      "        [[ 0.5748,  0.0660,  0.4087,  ...,  0.4005, -0.1963, -0.0752],\n",
      "         [ 0.4575,  0.3128,  0.1239,  ...,  0.1246, -0.2736, -0.2763]],\n",
      "\n",
      "        [[ 0.2520,  0.2381, -0.1136,  ...,  0.0835,  0.4746, -0.1767],\n",
      "         [ 0.1162, -0.1164, -0.1585,  ...,  0.2531,  0.5253, -0.2783]],\n",
      "\n",
      "        [[-0.3191,  0.2705, -0.3290,  ..., -0.3201,  0.2127,  0.0008],\n",
      "         [-0.2333,  0.2911, -0.3106,  ..., -0.3824,  0.1873, -0.0090]],\n",
      "\n",
      "        [[-0.1845,  0.4938, -0.0634,  ...,  0.2029,  0.4314,  0.5049],\n",
      "         [-0.4057,  0.4438,  0.0662,  ...,  0.1689,  0.2410,  0.5475]],\n",
      "\n",
      "        [[-0.0851,  0.0390, -0.2925,  ...,  0.3144, -0.1837,  0.3280],\n",
      "         [-0.0820, -0.2449, -0.1946,  ...,  0.0389, -0.0230,  0.2826]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def bidirectional_rnn_forward_multilayers(input, W_ih, W_hh, B_ih, B_hh, W_reverse_ih, W_reverse_hh, B_reverse_ih, B_reverse_hh):\n",
    "    h_0 = torch.zeros(1, 1, W_ih[0].shape[0])\n",
    "    h_n = []\n",
    "    \n",
    "    for layer in range(len(W_ih)):\n",
    "        output, h_prev = bidirectional_rnn_forward(input, W_ih[layer], W_hh[layer], B_ih[layer], B_hh[layer], h_0, W_reverse_ih[layer], W_reverse_hh[layer], B_reverse_ih[layer], B_reverse_hh[layer], h_0)\n",
    "        input = output\n",
    "        h_n.append(h_prev)\n",
    "    return output, torch.cat(h_n, axis=0)\n",
    "\n",
    "input = torch.rand(2, 32, 128) # N * L * D\n",
    "layer = nn.RNN(128, 256, 3, batch_first=True, bidirectional=True)\n",
    "torch_output, torch_h_n = layer(input)\n",
    "\n",
    "\n",
    "W_ih = [layer.weight_ih_l0, layer.weight_ih_l1, layer.weight_ih_l2]\n",
    "W_hh = [layer.weight_hh_l0, layer.weight_hh_l1, layer.weight_hh_l2] \n",
    "B_ih = [layer.bias_ih_l0, layer.bias_ih_l1, layer.bias_ih_l2]\n",
    "B_hh = [layer.bias_hh_l0, layer.bias_hh_l1, layer.bias_hh_l2]\n",
    "W_reverse_ih = [layer.weight_ih_l0_reverse, layer.weight_ih_l1_reverse, layer.weight_ih_l2_reverse]\n",
    "W_reverse_hh = [layer.weight_hh_l0_reverse, layer.weight_hh_l1_reverse, layer.weight_hh_l2_reverse] \n",
    "B_reverse_ih = [layer.bias_ih_l0_reverse, layer.bias_ih_l1_reverse, layer.bias_ih_l2_reverse]\n",
    "B_reverse_hh = [layer.bias_hh_l0_reverse, layer.bias_hh_l1_reverse, layer.bias_hh_l2_reverse]\n",
    "my_output, my_h_n = bidirectional_rnn_forward_multilayers(input, W_ih, W_hh, B_ih, B_hh, W_reverse_ih, W_reverse_hh, B_reverse_ih, B_reverse_hh)\n",
    "\n",
    "print(torch_output)\n",
    "print(my_output)\n",
    "\n",
    "print(torch_h_n)\n",
    "print(my_h_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "90c81fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 128])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for i in W_reverse_ih:\n",
    "    print(i.shape)\n",
    "for i in W_hh:\n",
    "    print(i.shape)\n",
    "for i in B_ih:\n",
    "    print(i.shape)\n",
    "for i in B_hh:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2c98476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, dims, hidden_dims, num_layers, bidirectional=False):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.W_ih = []\n",
    "        self.W_hh = []\n",
    "        self.B_ih = []\n",
    "        self.B_hh = []\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.W_ih_reverse = []\n",
    "            self.W_hh_reverse = []\n",
    "            self.B_ih_reverse = []\n",
    "            self.B_hh_reverse = []\n",
    "            for layer in range(0, num_layers):\n",
    "                if layer == 0:\n",
    "                    self.W_ih.append(torch.rand(hidden_dims, dims))\n",
    "                    self.W_ih_reverse.append(torch.rand(hidden_dims, dims))\n",
    "                else:\n",
    "                    self.W_ih.append(torch.rand(hidden_dims, 2 * hidden_dims))\n",
    "                    self.W_ih_reverse.append(torch.rand(hidden_dims, 2 * hidden_dims))\n",
    "                self.W_hh.append(torch.rand(hidden_dims, hidden_dims))\n",
    "                self.W_hh_reverse.append(torch.rand(hidden_dims, hidden_dims))\n",
    "                self.B_ih.append(torch.rand(hidden_dims, ))\n",
    "                self.B_ih_reverse.append(torch.rand(hidden_dims, ))\n",
    "                self.B_hh.append(torch.rand(hidden_dims, ))\n",
    "                self.B_hh_reverse.append(torch.rand(hidden_dims, ))\n",
    "            self.W_ih_reverse = nn.ParameterList(self.W_ih_reverse)\n",
    "            self.W_hh_reverse = nn.ParameterList(self.W_hh_reverse)\n",
    "            self.B_ih_reverse = nn.ParameterList(self.B_ih_reverse)\n",
    "            self.B_hh_reverse = nn.ParameterList(self.B_hh_reverse)\n",
    "        else:\n",
    "            for layer in range(0, num_layers):\n",
    "                if layer == 0:\n",
    "                    self.W_ih.append(torch.rand(hidden_dims, dims))\n",
    "                else:\n",
    "                    self.W_ih.append(torch.rand(hidden_dims, hidden_dims))\n",
    "                self.W_hh.append(torch.rand(hidden_dims, hidden_dims))\n",
    "                self.B_ih.append(torch.rand(hidden_dims, ))\n",
    "                self.B_hh.append(torch.rand(hidden_dims, ))\n",
    "                \n",
    "        self.W_ih = nn.ParameterList(self.W_ih)\n",
    "        self.W_hh = nn.ParameterList(self.W_hh)\n",
    "        self.B_ih = nn.ParameterList(self.B_ih)\n",
    "        self.B_hh = nn.ParameterList(self.B_hh)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        if self.bidirectional:\n",
    "            return bidirectional_rnn_forward_multilayers(x, self.W_ih, self.W_hh, self.B_ih, self.B_hh, self.W_ih_reverse, self.W_hh_reverse, self.B_ih_reverse, self.B_hh_reverse)\n",
    "        else:\n",
    "            return rnn_forward_multilayers(x, self.W_ih, self.W_hh, self.B_ih, self.B_hh)\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6781168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 512])\n",
      "torch.Size([6, 2, 256])\n",
      "W_ih_reverse.0\n",
      "W_ih_reverse.1\n",
      "W_ih_reverse.2\n",
      "W_hh_reverse.0\n",
      "W_hh_reverse.1\n",
      "W_hh_reverse.2\n",
      "B_ih_reverse.0\n",
      "B_ih_reverse.1\n",
      "B_ih_reverse.2\n",
      "B_hh_reverse.0\n",
      "B_hh_reverse.1\n",
      "B_hh_reverse.2\n",
      "W_ih.0\n",
      "W_ih.1\n",
      "W_ih.2\n",
      "W_hh.0\n",
      "W_hh.1\n",
      "W_hh.2\n",
      "B_ih.0\n",
      "B_ih.1\n",
      "B_ih.2\n",
      "B_hh.0\n",
      "B_hh.1\n",
      "B_hh.2\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(2, 32, 128) # N * L * D\n",
    "layer = RNN(128, 256, 3, bidirectional=True)\n",
    "my_output, my_h_n = layer(input)\n",
    "print(my_output.shape)\n",
    "print(my_h_n.shape)\n",
    "\n",
    "for k, v in layer.named_parameters():\n",
    "    print(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
