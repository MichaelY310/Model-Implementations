{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea72c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f508496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ae8683ffcd49ea85fb68a57bb3fa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/554 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7514e0bce942464997d63afb2e97b19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/525M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3e98ad5e934c2fbeab92845dc57ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/502M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c2c89e4099486d99454ce492be9518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/781M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3dff5296604755a4f1393b7b59394c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/62149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'caption'],\n",
      "    num_rows: 62149\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Isamu136/big-animal-dataset\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b5ac9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 100\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "362feb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "50\n",
      "{0: 'Cardigan', 1: 'chicken', 2: 'Blenheim_spaniel', 3: 'miniature_schnauzer', 4: 'butterfly', 5: 'leonberger', 6: 'Pembroke', 7: 'African_hunting_dog', 8: 'Abyssinian', 9: 'EntleBucher', 10: 'Pomeranian', 11: 'Norfolk_terrier', 12: 'wheaten terrier', 13: 'Newfoundland', 14: 'otterhound', 15: 'cocker_spaniel', 16: 'samoyed', 17: 'Pekinese', 18: 'groenendael', 19: 'dog', 20: 'cat', 21: 'miniature pinscher', 22: 'Birman', 23: 'keeshond', 24: 'Boston_bull', 25: 'beagle', 26: 'american pit bull terrier', 27: 'spider', 28: 'sheep', 29: 'boxer', 30: 'curly-coated_retriever', 31: 'Egyptian Mau', 32: 'Bouvier_des_Flandres', 33: 'elephant', 34: 'Samoyed', 35: 'basenji', 36: 'schipperke', 37: 'Maine Coon', 38: 'squirrel', 39: 'cow', 40: 'Norwich_terrier', 41: 'Bengal', 42: 'collie', 43: 'horse', 44: 'Irish_wolfhound', 45: 'dhole', 46: 'bluetick', 47: 'West_Highland_white_terrier', 48: 'Maltese_dog', 49: 'Shetland_sheepdog'}\n"
     ]
    }
   ],
   "source": [
    "print(train_size)\n",
    "caption_to_label = {}\n",
    "label_to_caption = {}\n",
    "\n",
    "caption_set = list(set(train_dataset[\"caption\"]))\n",
    "for i in range(0, len(caption_set)):\n",
    "    caption_to_label[caption_set[i]] = i\n",
    "    label_to_caption[i] = caption_set[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "num_classes = max(label_to_caption.keys()) + 1\n",
    "print(num_classes)\n",
    "print(label_to_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "976fae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),    # Resize to the target size\n",
    "    transforms.ToTensor(),          # Convert to a PyTorch tensor\n",
    "])\n",
    "\n",
    "class MyCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, your_data_here):\n",
    "        self.data = your_data_here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'image': image_transform(self.data['image'][idx]),\n",
    "            'caption': self.data['caption'][idx],\n",
    "            'label': caption_to_label[self.data['caption'][idx]],\n",
    "        }\n",
    "    \n",
    "# Finally create the dataloader\n",
    "my_train_dataset = MyCustomDataset(train_dataset)\n",
    "train_dataloader = torch.utils.data.DataLoader(my_train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fe047023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = torchvision.models.vision_transformer.VisionTransformer(        \n",
    "#         image_size = image_size,\n",
    "#         patch_size = 8,\n",
    "#         num_layers = 10,\n",
    "#         num_heads = 4,\n",
    "#         hidden_dim = 64,\n",
    "#         mlp_dim = 128,\n",
    "#         dropout = 0.2,\n",
    "#         attention_dropout = 0.2,\n",
    "#         num_classes = num_classes).to(device)\n",
    "model = torchvision.models.vit_b_16().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "652a7cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188631defcf14a6ea523f81ae32b6304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, iters)):\n\u001b[0;32m     13\u001b[0m     iter_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     15\u001b[0m         images \u001b[38;5;241m=\u001b[39m data_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[77], line 16\u001b[0m, in \u001b[0;36mMyCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image_transform(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[idx]),\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m][idx],\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: caption_to_label[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m][idx]],\n\u001b[0;32m     19\u001b[0m     }\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:2800\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2799\u001b[0m     \u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:2785\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2783\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2784\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2785\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\formatting\\formatting.py:398\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\formatting\\formatting.py:442\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    441\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_column(pa_table)\n\u001b[1;32m--> 442\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\formatting\\formatting.py:218\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_column\u001b[1;34m(self, column, column_name)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m column\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\features\\features.py:1951\u001b[0m, in \u001b[0;36mFeatures.decode_column\u001b[1;34m(self, column, column_name)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1939\u001b[0m     \u001b[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \n\u001b[0;32m   1941\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;124;03m        `list[Any]`\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1951\u001b[0m         [decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[0;32m   1952\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1953\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[0;32m   1954\u001b[0m     )\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\features\\features.py:1951\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1939\u001b[0m     \u001b[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \n\u001b[0;32m   1941\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;124;03m        `list[Any]`\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1951\u001b[0m         [\u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[0;32m   1952\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1953\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[0;32m   1954\u001b[0m     )\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\features\\features.py:1339\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[1;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[0;32m   1337\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[1;32m-> 1339\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\datasets\\features\\image.py:185\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[1;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[1;32m--> 185\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m         )\n\u001b[0;32m    256\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 257\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "images = None\n",
    "labels = None\n",
    "\n",
    "iters = 100\n",
    "loss_list = []\n",
    "for iter in tqdm(range(0, iters)):\n",
    "    iter_loss = 0\n",
    "    for i, data_batch in enumerate(train_dataloader):\n",
    "        images = data_batch[\"image\"].to(device)\n",
    "        print(images.shape)\n",
    "        labels = data_batch[\"label\"].to(device)\n",
    "        \n",
    "        pred = model(images)\n",
    "        loss = criterion(pred.float(), labels.long())\n",
    "        iter_loss += loss.data\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_list.append(iter_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "52a42d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21182f0ff40>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2e576f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, mlp_dim=3072):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, mlp_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(mlp_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MultiheadAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, p=0.2):\n",
    "        super(MultiheadAttentionBlock, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.linear = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        N = query.shape[0]\n",
    "\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        query = query.view(N, -1, self.num_heads, self.head_dim)\n",
    "        key = key.view(N, -1, self.num_heads, self.head_dim)\n",
    "        value = value.view(N, -1, self.num_heads, self.head_dim)\n",
    "\n",
    "        query = query.permute(0, 2, 1, 3)\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / torch.sqrt(\n",
    "            torch.tensor(self.head_dim, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(N, -1, self.embed_size)\n",
    "\n",
    "        output = self.linear(output)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_classes, image_size=224, patch_size=16, num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attention_layer = MultiheadAttentionBlock(hidden_dim, num_heads)\n",
    "#         self.attention_layer = nn.MultiheadAttention(hidden_dim, num_heads, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.mlp = MLP(hidden_dim, mlp_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.ln1(input)\n",
    "        x, _ = self.attention_layer(x, x, x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x + input\n",
    "        \n",
    "        y = self.ln2(x)\n",
    "        y = self.mlp(y)\n",
    "        return y + x\n",
    "\n",
    "        \n",
    "class MyVisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes, image_size=224, patch_size=16, num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072):\n",
    "        super(MyVisionTransformer, self).__init__()\n",
    "        \n",
    "        self.conv_layer = nn.Conv2d(in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        seq_len = (image_size // patch_size) ** 2 + 1  # we have a class token\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_len, hidden_dim).normal_(std=0.02))\n",
    "        self.encoder_dropout = nn.Dropout(0.2)\n",
    "        encoders = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            encoders[f\"encoder_layer_{i}\"] = EncoderBlock(num_classes, image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim)\n",
    "        self.encoders = nn.Sequential(encoders)\n",
    "        self.encoder_ln = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        \n",
    "        self.proj_layer = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, image_batch):\n",
    "        conv_result = self.conv_layer(image_batch)\n",
    "        embedding = conv_result.reshape(conv_result.shape[0], conv_result.shape[1], -1).permute(0, 2, 1)\n",
    "        batch_class_token = self.class_token.expand(image_batch.shape[0], -1, -1)\n",
    "\n",
    "        embedding = torch.cat([batch_class_token, embedding], dim=1)\n",
    "        embedding = embedding + self.pos_embedding\n",
    "        \n",
    "        encoder_output = self.encoder_ln(self.encoders(self.encoder_dropout(embedding)))\n",
    "        batch_class_token_output = encoder_output[:, 0, :]\n",
    "        \n",
    "        pred_prob = self.proj_layer(batch_class_token_output)\n",
    "        \n",
    "        return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c1b810ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyVisionTransformer(\n",
      "  (conv_layer): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (encoders): Sequential(\n",
      "    (encoder_layer_0): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_1): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_2): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_3): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_4): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_5): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_6): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_7): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_8): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_9): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_10): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_11): EncoderBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attention_layer): MultiheadAttentionBlock(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (proj_layer): Linear(in_features=768, out_features=50, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyVisionTransformer(num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d7860333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39dfd96e2274ba9996300ea6f7fcf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768, 14, 14])\n",
      "torch.Size([32, 196, 768])\n",
      "torch.Size([32, 1, 768])\n",
      "torch.Size([32, 197, 768])\n",
      "torch.Size([1, 197, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m images \u001b[38;5;241m=\u001b[39m data_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m data_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 18\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred\u001b[38;5;241m.\u001b[39mfloat(), labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     20\u001b[0m iter_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 120\u001b[0m, in \u001b[0;36mMyVisionTransformer.forward\u001b[1;34m(self, image_batch)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    118\u001b[0m embedding \u001b[38;5;241m=\u001b[39m embedding \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\n\u001b[1;32m--> 120\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_ln(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    121\u001b[0m batch_class_token_output \u001b[38;5;241m=\u001b[39m encoder_output[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m    123\u001b[0m pred_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_layer(batch_class_token_output)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 78\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 62\u001b[0m, in \u001b[0;36mMultiheadAttentionBlock.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     60\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_size)\n\u001b[1;32m---> 62\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, attention_weights\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "images = None\n",
    "labels = None\n",
    "\n",
    "iters = 100\n",
    "loss_list = []\n",
    "for iter in tqdm(range(0, iters)):\n",
    "    iter_loss = 0\n",
    "    for i, data_batch in enumerate(train_dataloader):\n",
    "        images = data_batch[\"image\"].to(device)\n",
    "        labels = data_batch[\"label\"].to(device)\n",
    "        \n",
    "        pred = model(images)\n",
    "        loss = criterion(pred.float(), labels.long())\n",
    "        iter_loss += loss.data\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_list.append(iter_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
