{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e210903a",
   "metadata": {},
   "source": [
    "https://github.com/rosinality/glow-pytorch/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9dcf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'E:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import scipy\n",
    "from math import log, pi, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31cb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Isamu136/big-animal-dataset\")\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9294cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 100\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f6b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_to_label = {}\n",
    "label_to_caption = {}\n",
    "\n",
    "caption_set = list(set(train_dataset[\"caption\"]))\n",
    "for i in range(0, len(caption_set)):\n",
    "    caption_to_label[caption_set[i]] = i\n",
    "    label_to_caption[i] = caption_set[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157d2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),    # Resize to the target size\n",
    "    transforms.ToTensor(),          # Convert to a PyTorch tensor\n",
    "])\n",
    "\n",
    "class MyCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, your_data_here):\n",
    "        self.data = your_data_here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['image'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'image': image_transform(self.data['image'][idx]),\n",
    "            'caption': self.data['caption'][idx],\n",
    "            'label': caption_to_label[self.data['caption'][idx]],\n",
    "        }\n",
    "    \n",
    "batch_size = 32\n",
    "my_train_dataset = MyCustomDataset(train_dataset)\n",
    "dataloader = torch.utils.data.DataLoader(my_train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca2a25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ActNorm, self).__init__()\n",
    "        self.loc = nn.Parameter(torch.zeros(1, in_channels, 1, 1))\n",
    "        self.scale = nn.Parameter(torch.zeros(1, in_channels, 1, 1))\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self, input):\n",
    "        mean = torch.mean(input, dim=(0, 2, 3), keepdim=True)\n",
    "        #tensor([[[[0.5439]], [[0.5019]], [[0.5298]]]])\n",
    "        std = torch.std(input, dim=(0, 2, 3), keepdim=True)\n",
    "    \n",
    "        self.loc.data.copy_(-mean)\n",
    "        self.scale.data.copy_(1 / (std + 1e-6))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if not self.initialized:\n",
    "            self.initialized = True\n",
    "            self.initialize(input)\n",
    "        \n",
    "        # the log(|abc|) = log(|a|) + log(|b|) + log(|c|)\n",
    "        logdet = input.shape[2] * input.shape[3] * torch.log(torch.abs(self.scale)).sum()\n",
    "        logdet = logdet.unsqueeze(0).repeat(input.shape[0])\n",
    "        \n",
    "        return (input / self.scale) - self.loc, logdet\n",
    "        \n",
    "    def reverse(self, input):\n",
    "        return self.scale * (input + self.loc)\n",
    "    \n",
    "    \n",
    "class InConv2d(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InConv2d, self).__init__()\n",
    "        self.eye = torch.eye(in_channels)\n",
    "        \n",
    "        kernel = np.random.randn(in_channels, in_channels)\n",
    "        # create a orthogonal matrix. A orthogonal matrix must be invertible\n",
    "        q, r = scipy.linalg.qr(kernel)  # QR factorization : Q: orthogonal. R: upper triangular        \n",
    "        p, l, u = scipy.linalg.lu(q.astype(np.float32))\n",
    "        s = np.diag(u)\n",
    "        u = np.triu(u, 1)\n",
    "        \n",
    "        p = torch.from_numpy(p)\n",
    "        l = torch.from_numpy(l)\n",
    "        u = torch.from_numpy(u)\n",
    "        s = torch.from_numpy(s)\n",
    "        \n",
    "        self.p = nn.Parameter(p, requires_grad=False)\n",
    "        self.l = nn.Parameter(l, requires_grad=True)\n",
    "        self.u = nn.Parameter(u, requires_grad=True)\n",
    "        self.s = nn.Parameter(s, requires_grad=True)\n",
    "        \n",
    "    def getWeight(self):\n",
    "        return self.p @ (torch.tril(self.l, 1) + self.eye) @ (torch.triu(self.u, 1) + torch.diag(self.s))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        weight = self.getWeight().unsqueeze(2).unsqueeze(3)\n",
    "        output = F.conv2d(input, weight)\n",
    "        \n",
    "        logdet = input.shape[2] * input.shape[3] * torch.log(torch.abs(self.s)).sum()\n",
    "        logdet = logdet.unsqueeze(0).repeat(input.shape[0])\n",
    "        \n",
    "        return output, logdet\n",
    "        \n",
    "    def reverse(self, input):\n",
    "        inv_weight = self.getWeight().inverse().unsqueeze(2).unsqueeze(3)\n",
    "        return F.conv2d(input, inv_weight)\n",
    "    \n",
    "    \n",
    "class ZeroConv2d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, padding=1):\n",
    "        super(ZeroConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, 3, padding=0)\n",
    "        self.conv.weight.data.zero_()\n",
    "        self.conv.bias.data.zero_()\n",
    "        self.scale = nn.Parameter(torch.zeros(1, out_channel, 1, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = F.pad(input, [1, 1, 1, 1], value=1)\n",
    "        out = self.conv(out)\n",
    "        out = out * torch.exp(self.scale * 3)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, in_channels, filter_size=512):\n",
    "        super(AffineCoupling, self).__init__()\n",
    "                \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels//2, filter_size, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filter_size, filter_size, 1),\n",
    "            nn.ReLU(),\n",
    "            ZeroConv2d(filter_size, in_channels, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layers[0].weight.data.normal_(0, 0.05)\n",
    "        self.layers[0].bias.data.normal_(0, 0.05)\n",
    "        self.layers[2].weight.data.normal_(0, 0.05)\n",
    "        self.layers[2].bias.data.normal_(0, 0.05)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # we will make in_channel divisible by 4 in the Block part\n",
    "        xa, xb= input.chunk(2, 1)\n",
    "            \n",
    "        log_s, t = self.layers(xb).chunk(2, 1)\n",
    "        s = F.sigmoid(log_s + 2)\n",
    "#         s = torch.exp(log_s)\n",
    "        ya = (xa + t) * s\n",
    "        yb = xb\n",
    "        output = torch.cat([ya, yb], dim=1)\n",
    "        \n",
    "        \n",
    "        # logdet\n",
    "        logdet = torch.log(torch.abs(s)).sum(1).sum(1).sum(1) # preserve batch\n",
    "        \n",
    "        \n",
    "        return output, logdet\n",
    "    \n",
    "    def reverse(self, input):\n",
    "        ya, yb= input.chunk(2, 1)\n",
    "            \n",
    "        log_s, t = self.layers(yb).chunk(2, 1)\n",
    "        s = F.sigmoid(log_s + 2)\n",
    "#         s = torch.exp(log_s)\n",
    "        xa = ya / s - t\n",
    "        xb = yb\n",
    "        output = torch.cat([xa, xb], dim=1)        \n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Flow, self).__init__()\n",
    "        self.actnorm = ActNorm(in_channels)\n",
    "        self.inconv2d = InConv2d(in_channels)\n",
    "        self.affinecoupling = AffineCoupling(in_channels)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out, det1 = self.actnorm(input)\n",
    "        out, det2 = self.inconv2d(out)\n",
    "        out, det3 = self.affinecoupling(out)\n",
    "        \n",
    "        return out, det1 + det2 + det3\n",
    "    \n",
    "    def reverse(self, input):\n",
    "        out = self.actnorm.reverse(input)\n",
    "        out = self.inconv2d.reverse(out)\n",
    "        out = self.affinecoupling.reverse(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, n_flow, split=True):\n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        self.split = split\n",
    "        \n",
    "        squeeze_dim = in_channels * 4\n",
    "        \n",
    "        self.flows = nn.ModuleList()\n",
    "        for i in range(n_flow):\n",
    "            self.flows.append(Flow(squeeze_dim))\n",
    "            \n",
    "        if split:\n",
    "            self.prior = ZeroConv2d(in_channels * 2, in_channels * 4)  # For learning the distribution of p_\\theta with model output\n",
    "        else:\n",
    "            self.prior = ZeroConv2d(in_channels * 4, in_channels * 8)  # For learning the distribution of p_\\theta with 0 \n",
    "                \n",
    "    def gaussian_log_p(self, x, mean, log_std):\n",
    "        return -0.5 * log(2 * pi) - log_std - 0.5 * (x - mean) ** 2 / torch.exp(2 * log_std)\n",
    "    \n",
    "    def gaussian_sample(self, eps, mean, log_std):\n",
    "        return mean + torch.exp(log_std) * eps\n",
    "        \n",
    "    def forward(self, input):\n",
    "        bs, in_channels, height, width = input.shape\n",
    "        # Enlarge in_channel so that it will be divisible by 4 in the AffineCoupling\n",
    "        squeezed_input = input.view(bs, in_channels, height//2, 2, width//2, 2)\n",
    "        squeezed_input = squeezed_input.permute(0, 1, 3, 5, 2, 4)\n",
    "        output = squeezed_input.contiguous().view(bs, in_channels * 4, height//2, width//2)\n",
    "#         output = squeezed_input.view(bs, in_channels * 4, height//2, width//2)\n",
    "        \n",
    "        total_logdet = 0\n",
    "        for flow in self.flows:\n",
    "            output, logdet = flow(output)\n",
    "            total_logdet = total_logdet + logdet\n",
    "            \n",
    "        if self.split:\n",
    "            # Output will be fed to be next layer\n",
    "            # new_z will be concatenated to reverse output to produce image\n",
    "            # into a linear layer to construct the prior of p_\\theta.\n",
    "            # We use Output as parameters of the distribution p_\\theta(z). Thus, the likelihood of z is maximized. \n",
    "            output, new_z = output.chunk(2, 1)\n",
    "            mean, log_std = self.prior(output).chunk(2, 1)\n",
    "            log_p_z = self.gaussian_log_p(new_z, mean, log_std)\n",
    "            log_p_z = log_p_z.sum(1).sum(1).sum(1)\n",
    "        else:\n",
    "            # When there is no z to split, we directly maximize the likelihood of Output\n",
    "            zero = torch.zeros_like(output)\n",
    "            mean, log_std = self.prior(zero).chunk(2, 1)\n",
    "            log_p_z = self.gaussian_log_p(output, mean, log_std)\n",
    "            log_p_z = log_p_z.sum(1).sum(1).sum(1)\n",
    "            new_z = output\n",
    "        \n",
    "        return output, total_logdet, new_z, log_p_z\n",
    "        \n",
    "    def reverse(self, input, eps=None):\n",
    "        if self.split:\n",
    "            mean, log_std = self.prior(input).chunk(2, 1)\n",
    "            z = self.gaussian_sample(eps, mean, log_std)  # use reparameterization trick to sample\n",
    "            output = torch.cat([input, z], dim=1)\n",
    "        else:\n",
    "            zero = torch.zeros_like(input)\n",
    "            mean, log_std = self.prior(zero).chunk(2, 1)\n",
    "            output = self.gaussian_sample(eps, mean, log_std)\n",
    "        \n",
    "        for flow in self.flows[::-1]:\n",
    "            output = flow.reverse(output)\n",
    "    \n",
    "        bs, in_channels, height, width = output.shape\n",
    "        unsqueezed_output = output.view(bs, in_channels // 4, 2, 2, height, width)\n",
    "        unsqueezed_output = unsqueezed_output.permute(0, 1, 4, 2, 5, 3)\n",
    "        unsqueezed_output = unsqueezed_output.contiguous().view(bs, in_channels // 4, height*2, width*2)\n",
    "#         unsqueezed_output = unsqueezed_output.view(bs, in_channels // 4, height*2, width*2)\n",
    "        \n",
    "        return unsqueezed_output\n",
    "\n",
    "\n",
    "class Glow(nn.Module):\n",
    "    def __init__(self, in_channels, n_flow, n_block):\n",
    "        super(Glow, self).__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        \n",
    "        # 3   ->  12  ->   6, 6   ->   6\n",
    "        # 6   ->  24  ->  12, 12  ->  12\n",
    "        # 12  ->  48  ->  48, 48  ->  48\n",
    "        \n",
    "        n_channels = in_channels\n",
    "        for i in range(0, n_block-1):\n",
    "            self.blocks.append(Block(n_channels, n_flow, split=True))\n",
    "            n_channels *= 2\n",
    "        self.blocks.append(Block(n_channels, n_flow, split=False))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        log_p_total = 0\n",
    "        logdet_total = 0\n",
    "        output = input\n",
    "        z_outs = []\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            output, total_logdet_, new_z, log_p_z = block(output)\n",
    "            log_p_total += log_p_z\n",
    "            logdet_total += total_logdet_\n",
    "            z_outs.append(new_z)\n",
    "            \n",
    "        return log_p_total, logdet_total, z_outs\n",
    "    \n",
    "    def reverse(self, z_outs):\n",
    "        output = input\n",
    "        for i, block in enumerate(self.blocks[::-1]):\n",
    "            if i == 0:\n",
    "                output = block.reverse(z_outs[-1], z_outs[-1])\n",
    "            else:\n",
    "                output = block.reverse(output, z_outs[-i-1])\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a55e83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bits = 5\n",
    "n_bins = 2.0 ** n_bits\n",
    "\n",
    "n_channels = 3\n",
    "n_flow = 32\n",
    "n_block = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Glow(n_channels, n_flow, n_block).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b844937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d340b0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m iter_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     52\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "# For generating images, we need to make a random z\n",
    "def calc_z_shapes(n_channel, input_size, n_flow, n_block):\n",
    "    z_shapes = []\n",
    "    for i in range(n_block - 1):\n",
    "        input_size //= 2\n",
    "        n_channel *= 2\n",
    "        z_shapes.append((n_channel, input_size, input_size))\n",
    "\n",
    "    input_size //= 2\n",
    "    z_shapes.append((n_channel * 4, input_size, input_size))\n",
    "    return z_shapes\n",
    "\n",
    "def calc_loss(log_p, logdet, image_size, n_bins):\n",
    "    n_pixel = image_size * image_size * 3\n",
    "\n",
    "    loss = -log(n_bins) * n_pixel\n",
    "    loss = loss + logdet + log_p\n",
    "\n",
    "    return (\n",
    "        (-loss / (log(2) * n_pixel)).mean(),\n",
    "        (log_p / (log(2) * n_pixel)).mean(),\n",
    "        (logdet / (log(2) * n_pixel)).mean(),\n",
    "    )\n",
    "\n",
    "\n",
    "iters = 100\n",
    "loss_list = []\n",
    "\n",
    "steps = 0\n",
    "for iter in tqdm(range(iters)):\n",
    "    iter_loss = 0\n",
    "    for data_batch in dataloader:\n",
    "        images = data_batch[\"image\"].to(device)\n",
    "        images = torch.floor(images * 255 / 2 ** (8 - n_bits)) / n_bins - 0.5\n",
    "        \n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                log_p, logdet, z_outs = model(images + torch.rand_like(images) / n_bins)\n",
    "                continue\n",
    "        else:\n",
    "            log_p, logdet, z_outs = model(images + torch.rand_like(images) / n_bins)\n",
    "        \n",
    "        logdet = logdet.mean()\n",
    "        loss, log_p, log_det = calc_loss(log_p, logdet, image_size, n_bins)\n",
    "        iter_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            model.eval()\n",
    "#             generate_image()\n",
    "            model.train()\n",
    "    loss_list.append(iter_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "837833bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\pyplot.py:2740\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2738\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   2741\u001b[0m         \u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39mscalex, scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   2742\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1662\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1663\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1664\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(iters), loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37851ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64, 64, 3)\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_image():\n",
    "    z_sample = []\n",
    "    z_shapes = calc_z_shapes(3, image_size, n_flow, n_block)\n",
    "    for z in z_shapes:\n",
    "        z_new = torch.randn(4, *z) * 98098\n",
    "        z_sample.append(z_new.to(device))\n",
    "    with torch.no_grad():\n",
    "        generated_images = model.reverse(z_sample).permute(0, 2, 3, 1).cpu().data.numpy()\n",
    "\n",
    "    print(generated_images.shape)\n",
    "    print(generated_images)\n",
    "    plt.figure(steps)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(4):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(generated_images[i])\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "generate_image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
